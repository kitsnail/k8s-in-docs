# Workload Variant Autoscaler - é¥±å’Œåº¦æ„ŸçŸ¥å¼¹æ€§ä¼¸ç¼©

> **æ ¸å¿ƒä»·å€¼**: è¶…è¶Š CPU/å†…å­˜æŒ‡æ ‡,åŸºäº LLM æ¨ç†é¥±å’Œåº¦å®ç°æ™ºèƒ½æ‰©ç¼©å®¹  
> **æŠ€æœ¯æ ˆ**: Go + Prometheus + HPA/KEDA  
> **å…³é”®æŒ‡æ ‡**: Scale-to-Zero å†·å¯åŠ¨ <30s, é¥±å’Œåº¦æ„ŸçŸ¥é¿å… SLO è¿çº¦

---

## ğŸŒ€ èºæ—‹ 1: ä¸ºä»€ä¹ˆ HPA å¯¹ LLM ä¸å¤Ÿæ™ºèƒ½?

### ä¼ ç»Ÿ HPA çš„ç›²ç‚¹

```yaml
# ä¼ ç»Ÿ HPA é…ç½®
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
spec:
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          averageUtilization: 70  # âŒ GPU æ¨ç† CPU åˆ©ç”¨ç‡ä½!
```

**é—®é¢˜**:
- GPU æ¨ç†: CPU åˆ©ç”¨ç‡ <20%, ä½† GPU å·²æ»¡è½½
- KV Cache å‹åŠ›: å†…å­˜å ç”¨ä¸ç­‰äºæ¨ç†èƒ½åŠ›
- é˜Ÿåˆ—å †ç§¯: HPA ä¸æ„ŸçŸ¥è¯·æ±‚ç­‰å¾…æ—¶é—´

---

### WVA çš„é¥±å’Œåº¦æ¨¡å‹

```python
saturation = (
    kv_memory_utilization * 0.5 +   # KV Cache å ç”¨
    queue_depth_ratio * 0.3 +        # é˜Ÿåˆ—æ·±åº¦
    throughput_degradation * 0.2     # ååä¸‹é™
)

if saturation > 0.8:
    desired_replicas = current + 1  # æ‰©å®¹
elif saturation < 0.3:
    desired_replicas = current - 1  # ç¼©å®¹
```

---

## ğŸ’¨ è®¤çŸ¥é™å‹

ç±»æ¯”å·¥å‚"å¼¹æ€§ç”¨å·¥":
- âŒ ä¼ ç»Ÿæ–¹æ¡ˆ: æ ¹æ®å·¥äºº"æ‰“å¡æ—¶é—´"åˆ¤æ–­æ˜¯å¦åŠ ç­ (ç±»æ¯” CPU åˆ©ç”¨ç‡)
- âœ… WVA æ–¹æ¡ˆ: æ ¹æ®"äº§çº¿é¥±å’Œåº¦"(åŸæ–™åº“å­˜+åœ¨åˆ¶å“+äº§èƒ½ä¸‹é™) å†³å®šæ‹›å·¥

---

## ğŸŒ€ èºæ—‹ 2: ä¸ HPA/KEDA é›†æˆ

### å·¥ä½œæµç¨‹

```mermaid
flowchart LR
    WVA[WVA Controller] -->|è®¡ç®—æœŸæœ›å‰¯æœ¬æ•°| PM[Prometheus Metrics]
    PM -->|wva_desired_replicas| HPA[HPA/KEDA]
    HPA -->|è°ƒæ•´å‰¯æœ¬| Deploy[Deployment]
    Deploy -.åé¦ˆ.-> WVA
```

---

## ğŸŒ€ èºæ—‹ 3: Scale-to-Zero é…ç½®

```yaml
apiVersion: llmd.ai/v1alpha1
kind: VariantAutoscaling
metadata:
  name: llama-autoscaler
spec:
  scaleTargetRef:
    kind: Deployment
    name: llama-70b
  saturationThreshold: 0.8
  scaleToZero:
    enabled: true
    idleTimeout: 300s  # 5 åˆ†é’Ÿæ— è¯·æ±‚ç¼©å®¹åˆ° 0
```

**é€‚ç”¨åœºæ™¯**: å†…éƒ¨å·¥å…·ã€å¼€å‘ç¯å¢ƒã€é—´æ­‡æ€§æ‰¹å¤„ç†

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [WVA Architecture](https://llm-d.ai/docs/architecture/Components/workload-variant-autoscaler)
- [Saturation Scaling Design](https://docs.google.com/document/d/1iGHqdxRUDpiKwtJFr5tMCKM7RF6fbTfZBL7BTn6UkwA/edit)
