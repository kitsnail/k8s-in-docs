# KV Cache Management - åˆ†å±‚ç¼“å­˜æ¶æ„

> **æ ¸å¿ƒä»·å€¼**: çªç ´å•æœºå†…å­˜é™åˆ¶,é€šè¿‡ GPU-CPU-FS ä¸‰çº§å­˜å‚¨å®ç° 10x+ å¹¶å‘èƒ½åŠ›  
> **æŠ€æœ¯æ ˆ**: vLLM KVConnector + POSIX I/O + LRU  
> **å…³é”®æŒ‡æ ‡**: å¹¶å‘ç”¨æˆ· 50â†’250 (13.9x), ååä¿æŒ 185k tok/s

---

## ğŸŒ€ èºæ—‹ 1: æ¦‚å¿µå±‚

### Transformer çš„"è®°å¿†ç“¶é¢ˆ"

**é—®é¢˜**: KV Cache å†…å­˜éœ€æ±‚ä¸ä¸Šä¸‹æ–‡é•¿åº¦çº¿æ€§å¢é•¿

```python
# Llama-3.1-70B å•ä¸ª Token çš„ KV Cache å¤§å°
kv_size_per_token = (
    80 layers * 2 (K+V) * 8192 hidden_dim * 2 bytes (FP16)
) = 2.56 MB/token

# 8k ä¸Šä¸‹æ–‡å¯¹è¯
single_conversation = 8192 * 2.56 MB = 20.97 GB

# H100 80GB HBM åªèƒ½æ”¯æŒ
max_concurrent = 80 GB / 21 GB â‰ˆ 3 ä¸ªé•¿å¯¹è¯! âŒ
```

**æ ¸å¿ƒçŸ›ç›¾**: GPU HBM æ˜¯æ¨ç†æ€§èƒ½å…³é”®,ä½†å®¹é‡æå…¶æœ‰é™

---

### llm-d çš„ä¸‰çº§å­˜å‚¨æ–¹æ¡ˆ

ç±»æ¯”å·¥å‚ä»“å‚¨ç³»ç»Ÿ:

| å­˜å‚¨å±‚ | å·¥å‚ç±»æ¯” | å®¹é‡ | å»¶è¿Ÿ | æˆæœ¬ |
|-------|---------|------|------|------|
| **L1: GPU HBM** | é«˜é€Ÿåº“å­˜ (äº§çº¿æ—) | 80 GB | <1Î¼s | ğŸ’°ğŸ’°ğŸ’° |
| **L2: CPU DRAM** | ä¸­è½¬ä»“ (è½¦é—´å†…) | 512 GB | ~10Î¼s | ğŸ’°ğŸ’° |
| **L3: æ–‡ä»¶ç³»ç»Ÿ** | å¤§å‹ä»“åº“ (å›­åŒºå¤–) | 10 TB+ | ~1ms | ğŸ’° |

**è®¾è®¡åŸåˆ™**:
- **çƒ­æ•°æ®åœ¨ GPU**: æ´»è·ƒä¼šè¯å¸¸é©» HBM
- **æ¸©æ•°æ®åœ¨ CPU**: å¾…æ¿€æ´»ä¼šè¯ç¼“å­˜ DRAM
- **å†·æ•°æ®åœ¨ç£ç›˜**: å†å²ä¼šè¯å½’æ¡£ FS,å¯è·¨èŠ‚ç‚¹å¤ç”¨

---

## ğŸ’¨ è®¤çŸ¥é™å‹

æƒ³è±¡è¶…å¸‚çš„"å‰ç½®ä»“"æ¨¡å¼:
- **æ”¶é“¶å° (GPU)**: åªæ”¾ç•…é”€å•†å“,å¿«é€Ÿç»“è´¦
- **åä»“ (CPU)**: å­˜æ”¾æ¬¡ç•…é”€å“,éœ€è¦æ—¶å¿«é€Ÿè¡¥è´§
- **é…é€ä¸­å¿ƒ (FS)**: å¤§é‡åº“å­˜,éš”å¤©é…é€

å…³é”®: **å¼‚æ­¥è¡¥è´§ä¸é˜»å¡ç»“è´¦** (å¯¹åº”å¼‚æ­¥ I/O ä¸é˜»å¡æ¨ç†)

---

## ğŸŒ€ èºæ—‹ 2: æœºåˆ¶å±‚

### vLLM KVConnector æŠ½è±¡å±‚

```python
class KVConnector:
    def put(self, block_id: int, data: Tensor, tier: str):
        """å¸è½½ KV Block åˆ°æŒ‡å®šå±‚"""
        if tier == "cpu":
            async_copy_to_cpu(data)
        elif tier == "fs":
            async_write_to_file(block_id, data)
    
    def get(self, block_id: int, tier: str) -> Tensor:
        """ä»æŒ‡å®šå±‚åŠ è½½ KV Block"""
        if tier == "cpu":
            return async_copy_from_cpu(block_id)
        elif tier == "fs":
            return async_read_from_file(block_id)
```

### é©±é€ä¸é¢„å–ç­–ç•¥

```mermaid
stateDiagram-v2
    [*] --> GPU_Hot: Prefill åˆ›å»º
    GPU_Hot --> CPU_Warm: GPU å†…å­˜ä¸è¶³<br/>(LRU é©±é€)
    CPU_Warm --> FS_Cold: CPU å†…å­˜ä¸è¶³
    
    FS_Cold --> CPU_Warm: ç”¨æˆ·å†æ¬¡è®¿é—®<br/>(é¢„å–)
    CPU_Warm --> GPU_Hot: æ¨ç†æ¿€æ´»
    
    GPU_Hot --> [*]: ä¼šè¯ç»“æŸ<br/>(åˆ é™¤)
    
    note right of GPU_Hot: å»¶è¿Ÿ <1Î¼s
    note right of CPU_Warm: å»¶è¿Ÿ ~10Î¼s
    note right of FS_Cold: å»¶è¿Ÿ ~1ms
```

---

## ğŸŒ€ èºæ—‹ 3: å®æˆ˜å±‚

### é…ç½®ç¤ºä¾‹

```yaml
# vLLM Deployment
env:
  - name: VLLM_KV_CACHE_OFFLOAD
    value: "cpu,fs"  # å¯ç”¨ä¸¤çº§å¸è½½
  
  - name: VLLM_GPU_MEMORY_UTILIZATION
    value: "0.85"  # é¢„ç•™ç©ºé—´ç»™ KV Cache
  
  - name: VLLM_KV_OFFLOAD_PATH
    value: "/mnt/shared-storage"  # Lustre/NFS
```

### ä½•æ—¶å¯ç”¨åˆ†å±‚ç¼“å­˜?

| åœºæ™¯ | å»ºè®® | ç†ç”± |
|------|------|------|
| å¹¶å‘ <20 ç”¨æˆ· | ä»… GPU | æ— å†…å­˜å‹åŠ› |
| å¹¶å‘ 20-100 ç”¨æˆ· | GPU + CPU | æˆæœ¬æœ€ä¼˜ |
| å¹¶å‘ >100 ç”¨æˆ· | GPU + CPU + FS | å¿…éœ€ |
| é•¿å¯¹è¯ (>16k) | GPU + FS | CPU å®¹é‡ä¸è¶³ |

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [vLLM KV Offloading Connector Blog](https://blog.vllm.ai/2026/01/08/kv-offloading-connector.html)
- [llm-d v0.5 Hierarchical KV Caching](https://llm-d.ai/blog/llm-d-v0.5-sustaining-performance-at-scale#optimized-offloading-hierarchical-kv-caching)
