# llm-d åˆ†å¸ƒå¼æ¨ç†å¹³å°æ¶æ„

> **æ ¸å¿ƒä»·å€¼**: Kubernetes åŸç”Ÿçš„å¤§æ¨¡å‹æ¨ç†å¹³å°,å®ç° SOTA æ€§èƒ½ä¸ç”Ÿäº§çº§è¿ç»´èƒ½åŠ›çš„ç»Ÿä¸€  
> **æŠ€æœ¯æ ˆ**: vLLM + Envoy + Kubernetes Gateway API + Prometheus  
> **é€‚ç”¨åœºæ™¯**: 70B+ å‚æ•°å¤§æ¨¡å‹ç”Ÿäº§æ¨ç†ã€é«˜ Prefix å¤ç”¨å·¥ä½œè´Ÿè½½ã€æ··åˆ SLO å¤šç§Ÿæˆ·æœåŠ¡

---

## ğŸŒ€ èºæ—‹ 1: æ¦‚å¿µå±‚ - LLM æ¨ç†çš„ç”Ÿäº§å›°å¢ƒä¸ llm-d çš„ä»·å€¼ä¸»å¼ 

### æœ¬å±‚ç›®æ ‡
å»ºç«‹å¯¹ LLM æ¨ç†ç”Ÿäº§åŒ–ç—›ç‚¹çš„è®¤çŸ¥,ç†è§£ llm-d å¦‚ä½•é€šè¿‡ Kubernetes åŸç”Ÿæ¶æ„è§£å†³è¿™äº›é—®é¢˜ã€‚

---

### 1.1 ä¸ºä»€ä¹ˆ LLM æ¨ç†æ¯”ä¼ ç»ŸæœåŠ¡æ›´éš¾?

ä¼ ç»Ÿå¾®æœåŠ¡çš„è´Ÿè½½å‡è¡¡å‡è®¾:
- âœ… è¯·æ±‚å¤„ç†æ—¶é—´ç›¸å¯¹å‡åŒ€
- âœ… èµ„æºæ¶ˆè€—å¯é¢„æµ‹
- âœ… æ— çŠ¶æ€,å®ä¾‹é—´å®Œå…¨å¯¹ç­‰

**ä½† LLM æ¨ç†æ‰“ç ´äº†æ‰€æœ‰å‡è®¾:**

| ç»´åº¦ | ä¼ ç»ŸæœåŠ¡ | LLM æ¨ç† |
|------|---------|---------|
| **è¯·æ±‚è€—æ—¶** | 10-100ms | 100ms-30s (å–å†³äºè¾“å…¥/è¾“å‡ºé•¿åº¦) |
| **å†…å­˜éœ€æ±‚** | å›ºå®š | åŠ¨æ€å¢é•¿ (KV Cache å ç”¨ä¸ä¸Šä¸‹æ–‡é•¿åº¦çº¿æ€§ç›¸å…³) |
| **è®¡ç®—æ¨¡å¼** | CPU å¯†é›†å‹ | Prefill è®¡ç®—å¯†é›† + Decode å†…å­˜å¸¦å®½å¯†é›† |
| **çŠ¶æ€ç®¡ç†** | æ— çŠ¶æ€ | æœ‰çŠ¶æ€ (KV Cache å¤ç”¨å¯å‡å°‘ 90% è®¡ç®—) |

**æ ¸å¿ƒçŸ›ç›¾**: 
- **å†…å­˜å¢™**: å•å¼  H100 80GB HBM åªèƒ½æœåŠ¡ ~20 ä¸ªå¹¶å‘é•¿å¯¹è¯ (Llama-70B)
- **å»¶è¿ŸæŠ–åŠ¨**: Round-robin å°†é•¿çŸ­è¯·æ±‚å‡åŒ€åˆ†é…,å¯¼è‡´å¤´éƒ¨è¯·æ±‚é˜»å¡é˜Ÿåˆ—
- **èµ„æºæµªè´¹**: Prefill åƒæ»¡ç®—åŠ›æ—¶ Decode åœ¨ç­‰å¾…,Decode åƒæ»¡å¸¦å®½æ—¶ Prefill åœ¨ç©ºè½¬

---

### 1.2 llm-d çš„æ ¸å¿ƒä»·å€¼ä¸»å¼ 

**å®šä½**: Kubernetes åŸç”Ÿçš„åˆ†å¸ƒå¼ LLM æ¨ç†æ§åˆ¶å¹³é¢

llm-d ä¸æ˜¯æ–°çš„æ¨ç†å¼•æ“,è€Œæ˜¯å°† **vLLM** (ä¸šç•Œæœ€å¿«æ¨ç†å¼•æ“) ä¸ **Kubernetes** (äº‘åŸç”Ÿç¼–æ’æ ‡å‡†) æ·±åº¦æ•´åˆ,æä¾›ä¸‰å¤§æ ¸å¿ƒèƒ½åŠ›:

#### ğŸ¯ èƒ½åŠ› 1: æ™ºèƒ½è°ƒåº¦ - è®©æ¯ä¸ªè¯·æ±‚æ‰¾åˆ°"æœ€åˆé€‚"çš„ GPU

ä¼ ç»Ÿ K8s Service çš„ Round-robin:
```
è¯·æ±‚ A (çŸ­) â†’ GPU 1 (é˜Ÿåˆ—: [é•¿, é•¿, é•¿])  âŒ æ’é˜Ÿç­‰å¾…
è¯·æ±‚ B (é•¿) â†’ GPU 2 (é˜Ÿåˆ—: [çŸ­, çŸ­])      âŒ é˜»å¡åç»­
```

llm-d Inference Scheduler çš„æ™ºèƒ½è·¯ç”±:
```
è¯·æ±‚ A (çŸ­) â†’ GPU 2 (é˜Ÿåˆ—: [çŸ­, çŸ­])      âœ… å¿«é€Ÿå“åº”
è¯·æ±‚ B (é•¿) â†’ GPU 1 (é˜Ÿåˆ—: [é•¿, é•¿, é•¿])  âœ… æ‰¹å¤„ç†ä¼˜åŒ–
è¯·æ±‚ C (æœ‰ç¼“å­˜) â†’ GPU 3 (ç¼“å­˜å‘½ä¸­ç‡ 90%)  âœ… é›¶ç­‰å¾…
```

**æ”¶ç›Š**: 
- TTFT (é¦– Token å»¶è¿Ÿ) é™ä½ **99%** (6s â†’ 60ms)
- ååæå‡ **109%** (å®æµ‹ Qwen3-32B)

---

#### ğŸ¯ èƒ½åŠ› 2: åˆ†å±‚ç¼“å­˜ - çªç ´å•æœºå†…å­˜é™åˆ¶

**é—®é¢˜**: GPU HBM æ˜¯ç¨€ç¼ºèµ„æº,KV Cache å ç”¨å¯¼è‡´å¹¶å‘ä¸Šé™ä½

**llm-d æ–¹æ¡ˆ**: ä¸‰çº§å­˜å‚¨å±‚æ¬¡ (ç±»æ¯”å·¥å‚ä»“å‚¨ç³»ç»Ÿ)

```mermaid
flowchart LR
    A[GPU HBM<br/>é«˜é€Ÿåº“å­˜] -->|æº¢å‡º| B[CPU DRAM<br/>ä¸­è½¬ä»“]
    B -->|å†·æ•°æ®| C[æ–‡ä»¶ç³»ç»Ÿ<br/>å¤§å‹ä»“åº“]
    C -.å¼‚æ­¥åŠ è½½.-> B
    B -.é¢„çƒ­.-> A
    
    style A fill:#e1f5fe
    style B fill:#fff9c4
    style C fill:#f3e5f5
```

**å®æµ‹æ•ˆæœ** (Llama-3.1-70B, IBM Storage Scale):
- çº¯ GPU: 50 å¹¶å‘ç”¨æˆ·æ—¶æ€§èƒ½å´©æºƒ
- GPU+CPU+FS: **250 å¹¶å‘ç”¨æˆ·ä»ä¿æŒ 185k tok/s** (13.9x æå‡)

---

#### ğŸ¯ èƒ½åŠ› 3: P/D åˆ†ç¦» - ä¸“ä¸šåŒ–åˆ†å·¥æå‡æ•ˆç‡

**æ ¸å¿ƒæ´å¯Ÿ**: Prefill (é¢„å¤„ç†) ä¸ Decode (ç”Ÿæˆ) çš„èµ„æºéœ€æ±‚æˆªç„¶ä¸åŒ

| é˜¶æ®µ | è®¡ç®—ç‰¹æ€§ | æœ€ä¼˜ç¡¬ä»¶é…ç½® |
|------|---------|-------------|
| **Prefill** | å¤§çŸ©é˜µè¿ç®—,è®¡ç®—å¯†é›† | å°‘ TP (Tensor Parallel),å¤šå‰¯æœ¬ |
| **Decode** | é€ Token ç”Ÿæˆ,å¸¦å®½å¯†é›† | é«˜ TP (å¤§æ˜¾å­˜),å°‘å‰¯æœ¬ |

**ä¼ ç»Ÿæ–¹æ¡ˆ**: åŒä¸€ Pod å¤„ç†ä¸¤ä¸ªé˜¶æ®µ â†’ èµ„æºåˆ©ç”¨ç‡ä½

**llm-d P/D Disaggregation**:
```mermaid
flowchart LR
    Req[è¯·æ±‚] --> PS[Prefill å®ä¾‹<br/>TP=1, å‰¯æœ¬=4<br/>è®¡ç®—ä¼˜åŒ–]
    PS -->|KV Cache ä¼ è¾“<br/>RDMA/UCCL| DS[Decode å®ä¾‹<br/>TP=4, å‰¯æœ¬=1<br/>å¸¦å®½ä¼˜åŒ–]
    DS --> Resp[å“åº”æµ]
    
    style PS fill:#e1f5fe
    style DS fill:#fff3e0
```

**é€‚ç”¨åœºæ™¯**: 
- è¶…å¤§æ¨¡å‹ (120B+)
- é•¿ä¸Šä¸‹æ–‡ (10k+ input tokens)
- MoE æ¶æ„ (DeepSeek-R1)

---

### 1.3 ç³»ç»Ÿå…¨æ™¯æ¶æ„ - ğŸ­ æ™ºèƒ½å·¥å‚ç±»æ¯”

å°† llm-d ç±»æ¯”ä¸º**ç°ä»£åŒ–æ™ºèƒ½å·¥å‚çš„ç”Ÿäº§è°ƒåº¦ç³»ç»Ÿ**:

```mermaid
flowchart TB
    subgraph è®¢å•ä¸­å¿ƒ [ğŸ¯ è®¢å•ä¸­å¿ƒ - Gateway]
        GW[Istio/Envoy<br/>è®¢å•å…¥å£]
    end
    
    subgraph è°ƒåº¦ä¸­å¿ƒ [ğŸ§  è°ƒåº¦ä¸­å¿ƒ - Inference Scheduler]
        EPP[æ™ºèƒ½è°ƒåº¦å™¨<br/>Filterâ†’Scoreâ†’Select]
    end
    
    subgraph ç”Ÿäº§è½¦é—´ [ğŸ­ ç”Ÿäº§è½¦é—´ - vLLM Pods]
        P1[é¢„å¤„ç†è½¦é—´ 1<br/>Prefill Pod]
        P2[é¢„å¤„ç†è½¦é—´ 2<br/>Prefill Pod]
        D1[ç²¾åŠ å·¥äº§çº¿<br/>Decode Pod]
    end
    
    subgraph ä»“å‚¨ç³»ç»Ÿ [ğŸ“¦ ä»“å‚¨ç³»ç»Ÿ - KV Cache]
        L1[é«˜é€Ÿåº“å­˜<br/>GPU HBM]
        L2[ä¸­è½¬ä»“<br/>CPU DRAM]
        L3[å¤§å‹ä»“åº“<br/>æ–‡ä»¶ç³»ç»Ÿ]
    end
    
    subgraph ç‰©æµç½‘ç»œ [ğŸšš ç‰©æµç½‘ç»œ - NIXL/UCCL]
        NET[ä¸“çº¿è¿è¾“<br/>RDMA/TCP-X]
    end
    
    subgraph ç”¨å·¥è°ƒåº¦ [ğŸ“Š ç”¨å·¥è°ƒåº¦ - Autoscaler]
        WVA[é¥±å’Œåº¦ç›‘æ§<br/>å¼¹æ€§ç”¨å·¥]
    end
    
    GW -->|è®¢å•| EPP
    EPP -->|ä»»åŠ¡åˆ†é…| P1 & P2 & D1
    P1 & P2 -.KV è½¬ç§».-> NET
    NET -.-> D1
    P1 & P2 & D1 <-->|åº“å­˜ç®¡ç†| L1
    L1 <-->|æº¢å‡º/é¢„çƒ­| L2
    L2 <-->|å½’æ¡£/åŠ è½½| L3
    WVA -.ç›‘æ§.-> P1 & P2 & D1
    WVA -.æ‰©ç¼©å®¹.-> ç”Ÿäº§è½¦é—´
    
    style è°ƒåº¦ä¸­å¿ƒ fill:#e1f5fe
    style ç”Ÿäº§è½¦é—´ fill:#fff9c4
    style ä»“å‚¨ç³»ç»Ÿ fill:#f3e5f5
    style ç‰©æµç½‘ç»œ fill:#e8f5e9
```

**ç±»æ¯”æ˜ å°„è¡¨**:

| llm-d ç»„ä»¶ | å·¥å‚ç±»æ¯” | æ ¸å¿ƒèŒè´£ |
|-----------|---------|---------|
| **Gateway** | è®¢å•ä¸­å¿ƒ | æ¥æ”¶æ¨ç†è¯·æ±‚,ç»Ÿä¸€å…¥å£ |
| **Inference Scheduler** | ç”Ÿäº§è°ƒåº¦ä¸­å¿ƒ | æ ¹æ®è®¢å•ç‰¹æ€§ã€è½¦é—´è´Ÿè½½ã€åº“å­˜çŠ¶æ€æ™ºèƒ½åˆ†é… |
| **vLLM Pods (Prefill)** | é¢„å¤„ç†è½¦é—´ | åŸæ–™åŠ å·¥ (Prompt å¤„ç†) |
| **vLLM Pods (Decode)** | ç²¾åŠ å·¥äº§çº¿ | ç²¾å¯†ç»„è£… (Token ç”Ÿæˆ) |
| **KV Cache (GPU)** | é«˜é€Ÿåº“å­˜ | çƒ­æ•°æ®,æ¯«ç§’çº§è®¿é—® |
| **KV Cache (CPU)** | ä¸­è½¬ä»“ | æ¸©æ•°æ®,å¾®ç§’çº§è®¿é—® |
| **KV Cache (FS)** | å¤§å‹ä»“åº“ | å†·æ•°æ®,æ¯«ç§’çº§è®¿é—® |
| **NIXL/UCCL** | ç‰©æµä¸“çº¿ | ä¿è¯é¢„å¤„ç†åˆ°ç²¾åŠ å·¥çš„é«˜é€Ÿã€å¯é è¿è¾“ |
| **Autoscaler (WVA)** | å¼¹æ€§ç”¨å·¥è°ƒåº¦ | æ ¹æ®è®¢å•é‡ã€è½¦é—´é¥±å’Œåº¦åŠ¨æ€å¢å‡äº§çº¿ |

---

### âœ… èºæ—‹ 1 éªŒæ”¶æ ‡å‡†

å®Œæˆæœ¬å±‚å­¦ä¹ å,ä½ åº”è¯¥èƒ½å¤Ÿ:

- [ ] ç”¨ä¸€å¥è¯è¯´å‡º llm-d çš„æ ¸å¿ƒä»·å€¼: _"Kubernetes åŸç”Ÿçš„ LLM æ¨ç†æ§åˆ¶å¹³é¢,é€šè¿‡æ™ºèƒ½è°ƒåº¦ã€åˆ†å±‚ç¼“å­˜ã€P/D åˆ†ç¦»å®ç°ç”Ÿäº§çº§æ€§èƒ½ä¸è¿ç»´èƒ½åŠ›ç»Ÿä¸€"_
- [ ] åˆ—ä¸¾ LLM æ¨ç†ç›¸æ¯”ä¼ ç»ŸæœåŠ¡çš„ä¸‰å¤§å·®å¼‚: èµ„æºéœ€æ±‚ä¸å‡ã€çŠ¶æ€ä¾èµ–ã€è®¡ç®—æ¨¡å¼åˆ†é˜¶æ®µ
- [ ] ç”¨å·¥å‚ç±»æ¯”è§£é‡Š llm-d çš„å››å¤§ç»„ä»¶å¦‚ä½•ååŒå·¥ä½œ

---

### ğŸ”— ä¸‹ä¸€æ­¥

ç†è§£äº† llm-d çš„ä»·å€¼å®šä½å,ä¸‹ä¸€å±‚æˆ‘ä»¬å°†æ·±å…¥ **è°ƒåº¦å†³ç­–çš„ç®—æ³•æœºåˆ¶** ä¸ **å››å¤§ç»„ä»¶çš„ååŒæ—¶åº**ã€‚

---

## ğŸ’¨ è®¤çŸ¥é™å‹ - ä¸ºä»€ä¹ˆéœ€è¦"æ™ºèƒ½è°ƒåº¦"è€Œéç®€å•è´Ÿè½½å‡è¡¡?

### å¸¸è¯†ç±»æ¯”: åŒ»é™¢æŒ‚å·åˆ†è¯Šç³»ç»Ÿ

æƒ³è±¡ä½ å»ä¸‰ç”²åŒ»é™¢çœ‹ç—…:

**âŒ Round-robin è´Ÿè½½å‡è¡¡ (æŒ‰é¡ºåºæ’é˜Ÿ)**
```
æ‚£è€… A (é‡æ„Ÿå†’,5 åˆ†é’Ÿ) â†’ 1 å·è¯Šå®¤ (é˜Ÿåˆ—: [éª¨æŠ˜, é˜‘å°¾ç‚, è‚ºç‚])  
æ‚£è€… B (éª¨æŠ˜,30 åˆ†é’Ÿ)  â†’ 2 å·è¯Šå®¤ (é˜Ÿåˆ—: [æ„Ÿå†’, æ„Ÿå†’])  
æ‚£è€… C (å¤è¯Š,ç—…å†åœ¨ 1 å·è¯Šå®¤) â†’ 3 å·è¯Šå®¤ (éœ€è¦é‡æ–°æ£€æŸ¥)
```
**ç»“æœ**: é‡æ„Ÿå†’æ’é˜Ÿ 2 å°æ—¶,éª¨æŠ˜æ‚£è€…é˜»å¡å¿«é€Ÿé—¨è¯Š,å¤è¯Šæ‚£è€…é‡å¤æ£€æŸ¥æµªè´¹èµ„æº

---

**âœ… æ™ºèƒ½åˆ†è¯Šç³»ç»Ÿ (llm-d Scheduler)**
```
æ‚£è€… A (é‡æ„Ÿå†’)        â†’ å¿«é€Ÿé—¨è¯Š (é˜Ÿåˆ—: [æ„Ÿå†’, æ„Ÿå†’])
æ‚£è€… B (éª¨æŠ˜)          â†’ ä¸“ç§‘é—¨è¯Š (é˜Ÿåˆ—: [éª¨æŠ˜, æ‰‹æœ¯])
æ‚£è€… C (å¤è¯Š,æœ‰ç—…å†)   â†’ 1 å·è¯Šå®¤ (ç—…å†è°ƒç”¨,å¿«é€Ÿè¯Šæ–­)
```
**æ”¶ç›Š**: 
- é‡æ„Ÿå†’ 5 åˆ†é’Ÿæå®š
- éª¨æŠ˜æ‚£è€…åœ¨ä¸“ç§‘æ‰¹é‡å¤„ç†
- å¤è¯Šæ‚£è€…å¤ç”¨æ£€æŸ¥ç»“æœ

---

### æ˜ å°„åˆ° LLM æ¨ç†

| åŒ»é™¢åœºæ™¯ | LLM æ¨ç† |
|---------|---------|
| **æ‚£è€…ç±»å‹** | è¯·æ±‚é•¿åº¦ (çŸ­/é•¿ Prompt) |
| **ç—…å†** | KV Cache (å†å²ä¸Šä¸‹æ–‡) |
| **å¿«é€Ÿé—¨è¯Š** | ä½è´Ÿè½½ Pod |
| **ä¸“ç§‘é—¨è¯Š** | æ‰¹å¤„ç†ä¼˜åŒ– Pod |
| **åˆ†è¯ŠæŠ¤å£«** | Inference Scheduler |
| **ç—…å†è°ƒç”¨** | Prefix Cache Hit |

**æ ¸å¿ƒæ´å¯Ÿ**: 
- ä¸æ˜¯æ‰€æœ‰è¯·æ±‚éƒ½ä¸€æ ·é‡è¦/è€—æ—¶
- å¤ç”¨å†å²çŠ¶æ€ (ç—…å†/KV Cache) å¯ä»¥é¿å…é‡å¤å·¥ä½œ
- ä¸“ä¸šåŒ–åˆ†å·¥ (ä¸“ç§‘/P/D åˆ†ç¦») æå‡æ•´ä½“æ•ˆç‡

---

### ä¸ºä»€ä¹ˆ Kubernetes Service ä¸å¤Ÿç”¨?

Kubernetes Service çš„é»˜è®¤è¡Œä¸º:
```yaml
kind: Service
spec:
  type: LoadBalancer
  sessionAffinity: None  # æ— çŠ¶æ€å‡è®¾
```

**é—®é¢˜ 1: æ— æ³•æ„ŸçŸ¥è¯·æ±‚ç‰¹å¾**
- Service åªçœ‹ IP/ç«¯å£,ä¸çŸ¥é“è¿™æ˜¯ 10 Token è¿˜æ˜¯ 10k Token çš„è¯·æ±‚

**é—®é¢˜ 2: æ— æ³•æ„ŸçŸ¥åç«¯çŠ¶æ€**
- ä¸çŸ¥é“å“ªä¸ª Pod é˜Ÿåˆ—å·²æ»¡ã€å“ªä¸ª Pod æœ‰ç¼“å­˜å‘½ä¸­

**é—®é¢˜ 3: æ— æ³•åŠ¨æ€ä¼˜åŒ–**
- Round-robin å›ºå®šç®—æ³•,æ— æ³•æ ¹æ®å·¥ä½œè´Ÿè½½è°ƒæ•´

---

**llm-d çš„è§£å†³æ–¹æ¡ˆ**: åœ¨ Gateway å±‚æ’å…¥ **Endpoint Picker (EPP)** 
- ğŸ“Š è¯»å–è¯·æ±‚ Prompt â†’ è®¡ç®— Hash â†’ æŸ¥è¯¢ç¼“å­˜ç´¢å¼•
- ğŸ” è¿‡æ»¤ä¸å¯ç”¨ Pod (é˜Ÿåˆ—æ»¡ã€å†…å­˜ä¸è¶³)
- ğŸ¯ è¯„åˆ†æ’åº (ç¼“å­˜å‘½ä¸­ > è´Ÿè½½å‡è¡¡ > éšæœº)
- âœ… é€‰æ‹©æœ€ä¼˜ Pod å¤„ç†è¯·æ±‚

---

ç°åœ¨ä½ å·²ç»ç†è§£äº†"ä¸ºä»€ä¹ˆ"å’Œ"æ˜¯ä»€ä¹ˆ",ä¸‹ä¸€å±‚æˆ‘ä»¬å°†æ­å¼€è°ƒåº¦ç®—æ³•çš„åº•å±‚æœºåˆ¶ã€‚

---

## ğŸŒ€ èºæ—‹ 2: æœºåˆ¶å±‚ - å››å¤§æ”¯æŸ±çš„ååŒåŸç†

### æœ¬å±‚ç›®æ ‡
æ­ç¤º llm-d çš„æ ¸å¿ƒç®—æ³•ä¸æ•°æ®æµ,ç†è§£è°ƒåº¦å†³ç­–ã€ç¼“å­˜ç®¡ç†ã€P/D åä½œã€å¼¹æ€§ä¼¸ç¼©çš„åº•å±‚æœºåˆ¶ã€‚

---

### 2.1 æ ¸å¿ƒæ•°æ®æµ - ä»è¯·æ±‚åˆ°å“åº”çš„å®Œæ•´è·¯å¾„

```mermaid
sequenceDiagram
    participant C as å®¢æˆ·ç«¯
    participant GW as Gateway<br/>(Envoy)
    participant EPP as Endpoint Picker<br/>(è°ƒåº¦å™¨)
    participant IDX as KV Cache Indexer<br/>(ç¼“å­˜ç´¢å¼•)
    participant P as Prefill Pod<br/>(é¢„å¤„ç†)
    participant D as Decode Pod<br/>(ç”Ÿæˆ)
    participant WVA as Autoscaler<br/>(å¼¹æ€§ä¼¸ç¼©)
    
    C->>GW: 1. POST /v1/chat/completions
    GW->>EPP: 2. ext-proc è°ƒç”¨:<br/>è¯·æ±‚ Prompt + å…ƒæ•°æ®
    EPP->>IDX: 3. æŸ¥è¯¢ç¼“å­˜ç´¢å¼•:<br/>Hash(Prompt) â†’ Pod æ˜ å°„
    IDX-->>EPP: 4. è¿”å›è¯„åˆ†:<br/>Pod A (ç¼“å­˜ 90%), Pod B (é˜Ÿåˆ—ç©ºé—²)
    
    EPP->>EPP: 5. è°ƒåº¦å†³ç­–:<br/>Filter â†’ Score â†’ Select
    EPP-->>GW: 6. é€‰æ‹© Pod A (Prefill)
    
    GW->>P: 7. è½¬å‘è¯·æ±‚
    P->>P: 8. Prefill è®¡ç®— KV Cache
    P->>D: 9. NIXL ä¼ è¾“ KV Cache<br/>(RDMA/UCCL)
    D->>D: 10. Decode ç”Ÿæˆ Token
    D-->>C: 11. æµå¼è¿”å› Token
    
    D->>IDX: 12. KVEvents: æ›´æ–°ç¼“å­˜ç´¢å¼•
    D->>WVA: 13. Metrics: é˜Ÿåˆ—æ·±åº¦ã€KV åˆ©ç”¨ç‡
    WVA->>WVA: 14. è®¡ç®—é¥±å’Œåº¦ â†’ æ‰©ç¼©å®¹å†³ç­–
```

**å…³é”®èŠ‚ç‚¹è§£æ**:

- **æ­¥éª¤ 3-4**: ç¼“å­˜æ„ŸçŸ¥è·¯ç”± - åˆ©ç”¨ KV Cache Indexer çš„å…¨å±€è§†å›¾
- **æ­¥éª¤ 5**: ä¸‰é˜¶æ®µè°ƒåº¦ (è¯¦è§ä¸‹èŠ‚)
- **æ­¥éª¤ 9**: P/D åˆ†ç¦»çš„æ ¸å¿ƒ - é›¶æ‹·è´ KV ä¼ è¾“
- **æ­¥éª¤ 12**: å®æ—¶æ›´æ–°ç¼“å­˜ç´¢å¼• (ZeroMQ äº‹ä»¶æµ)
- **æ­¥éª¤ 13-14**: é—­ç¯åé¦ˆ - æ ¹æ®å®é™…è´Ÿè½½åŠ¨æ€æ‰©ç¼©å®¹

---

### 2.2 è°ƒåº¦ç®—æ³• - Filter â†’ Score â†’ Select ä¸‰é˜¶æ®µ

#### é˜¶æ®µ 1: Filter (è¿‡æ»¤ä¸å¯ç”¨èŠ‚ç‚¹)

æ’é™¤æ— æ³•å¤„ç†è¯·æ±‚çš„ Pod:

```python
# ä¼ªä»£ç 
def filter_pods(pods, request):
    available = []
    for pod in pods:
        if pod.queue_length > MAX_QUEUE:
            continue  # é˜Ÿåˆ—å·²æ»¡
        if pod.kv_memory_used > 0.95:
            continue  # KV Cache å†…å­˜ä¸è¶³
        if pod.model_id != request.model_id:
            continue  # æ¨¡å‹ä¸åŒ¹é…
        available.append(pod)
    return available
```

**å¸¸è§è¿‡æ»¤å™¨**:
- **Queue Depth Filter**: `queue_length < threshold`
- **Memory Pressure Filter**: `kv_utilization < 95%`
- **Model Compatibility Filter**: æ¨¡å‹ ID/LoRA Adapter åŒ¹é…

---

#### é˜¶æ®µ 2: Score (å¤šç»´åº¦è¯„åˆ†)

ä¸ºæ¯ä¸ªå¯ç”¨ Pod è®¡ç®—ç»¼åˆå¾—åˆ†:

```python
# ä¼ªä»£ç 
def score_pod(pod, request):
    score = 0
    
    # Scorer 1: Prefix Cache Hit (æƒé‡ 100)
    cache_hit_rate = calculate_cache_hit(pod, request.prompt)
    score += cache_hit_rate * 100
    
    # Scorer 2: Load Balancing (æƒé‡ 50)
    load_factor = 1 - (pod.queue_length / MAX_QUEUE)
    score += load_factor * 50
    
    # Scorer 3: Predicted Latency (å®éªŒæ€§,æƒé‡ 30)
    predicted_ttft = estimate_ttft(pod, request)
    score += (1 / predicted_ttft) * 30
    
    return score
```

**æ ¸å¿ƒ Scorer è¯¦è§£**:

| Scorer | è®¡ç®—æ–¹å¼ | é€‚ç”¨åœºæ™¯ |
|--------|---------|---------|
| **Prefix-aware** | Hash Block åŒ¹é…ç‡ | é«˜ Prefix å¤ç”¨ (RAG/å¤šè½®å¯¹è¯) |
| **Load-aware** | é˜Ÿåˆ—æ·±åº¦å€’æ•° | ä½ Prefix å¤ç”¨ (æ‰¹å¤„ç†) |
| **Predicted Latency** | TTFT/TPOT é¢„æµ‹æ¨¡å‹ | ä¸¥æ ¼ SLO åœºæ™¯ |
| **LoRA-aware** | Adapter æœ¬åœ°åŒ– | å¤šç§Ÿæˆ· LoRA æœåŠ¡ |

**æƒé‡é…ç½®ç¤ºä¾‹**:
```yaml
# é«˜ Prefix å¤ç”¨åœºæ™¯
scorers:
  - type: prefix-aware
    weight: 100
  - type: load-aware
    weight: 30

# ä½ Prefix å¤ç”¨åœºæ™¯
scorers:
  - type: load-aware
    weight: 100
```

---

#### é˜¶æ®µ 3: Select (é€‰æ‹©ä¸å®¹é”™)

```python
# ä¼ªä»£ç 
def select_pod(scored_pods):
    # æŒ‰å¾—åˆ†æ’åº
    sorted_pods = sort_by_score(scored_pods, descending=True)
    
    # Top-K é€‰æ‹© (æå‡é²æ£’æ€§)
    candidates = sorted_pods[:3]
    
    # éšæœºæ‰“æ•£ (é¿å…é›ªå´©)
    selected = random.choice(candidates)
    
    return selected
```

**å®¹é”™ç­–ç•¥**:
- **Top-K é€‰æ‹©**: ä¸æ€»æ˜¯é€‰ç¬¬ä¸€å,é¿å…å•ç‚¹è¿‡è½½
- **Fallback**: æ‰€æœ‰ Pod ä¸å¯ç”¨æ—¶é™çº§åˆ° Round-robin

---

### 2.3 KV Cache åˆ†å±‚ç®¡ç† - ä¸‰çº§å­˜å‚¨ååŒ

#### å±‚æ¬¡æ¶æ„

```mermaid
flowchart TB
    subgraph L1 [L1: GPU HBM - çƒ­æ•°æ®å±‚]
        direction LR
        G1[Block 0-100<br/>æ´»è·ƒä¼šè¯]
        G2[Block 101-200<br/>æœ€è¿‘è®¿é—®]
    end
    
    subgraph L2 [L2: CPU DRAM - æ¸©æ•°æ®å±‚]
        direction LR
        C1[Block 201-500<br/>å¾…æ¿€æ´»]
        C2[Block 501-800<br/>å‡†å¤‡å¸è½½]
    end
    
    subgraph L3 [L3: æ–‡ä»¶ç³»ç»Ÿ - å†·æ•°æ®å±‚]
        direction LR
        F1[Block 801-5000<br/>å†å²ä¼šè¯]
        F2[å…±äº«å­˜å‚¨<br/>è·¨èŠ‚ç‚¹å¤ç”¨]
    end
    
    G1 & G2 -->|æº¢å‡º| C1
    C1 -->|å½’æ¡£| F1
    F1 -.å¼‚æ­¥åŠ è½½.-> C1
    C1 -.é¢„çƒ­.-> G1
    
    style L1 fill:#e1f5fe
    style L2 fill:#fff9c4
    style L3 fill:#f3e5f5
```

#### æ ¸å¿ƒæœºåˆ¶

**1. vLLM KVConnector æŠ½è±¡å±‚**

```python
class KVConnector:
    def put(self, block_id: int, data: Tensor):
        """å¸è½½ KV Block åˆ°ä¸‹ä¸€å±‚"""
        pass
    
    def get(self, block_id: int) -> Tensor:
        """ä»ä¸‹å±‚åŠ è½½ KV Block"""
        pass
    
    def delete(self, block_id: int):
        """åˆ é™¤ä¸å†éœ€è¦çš„ Block"""
        pass
```

**2. å¼‚æ­¥ I/O æµæ°´çº¿**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ vLLM Engine (æ¨ç†ä¸»å¾ªç¯)             â”‚
â”‚  â”œâ”€ ç”Ÿæˆ Token (ä¸é˜»å¡)              â”‚
â”‚  â””â”€ è§¦å‘å¸è½½ (å¼‚æ­¥é˜Ÿåˆ—)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Offload Worker çº¿ç¨‹æ±                 â”‚
â”‚  â”œâ”€ ä» GPU æ‹·è´åˆ° CPU (CUDA Stream)  â”‚
â”‚  â”œâ”€ å‹ç¼©/åºåˆ—åŒ– (å¯é€‰)                â”‚
â”‚  â””â”€ å†™å…¥æ–‡ä»¶ç³»ç»Ÿ (å¹¶è¡Œ I/O)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**3. é©±é€ç­–ç•¥ (LRU)**

```python
# å½“ GPU HBM ä¸è¶³æ—¶
if gpu_memory_used > threshold:
    # é€‰æ‹©æœ€ä¹…æœªè®¿é—®çš„ Block
    victim_block = lru_cache.pop_least_recent()
    # å¼‚æ­¥å¸è½½åˆ° CPU
    offload_to_cpu(victim_block)
```

---

### 2.4 P/D åˆ†ç¦»çš„ç½‘ç»œä¼ è¾“ - NIXL/UCCL åè®®æ ˆ

#### ä¼ è¾“è·¯å¾„

```mermaid
flowchart LR
    subgraph Prefill [Prefill èŠ‚ç‚¹]
        P_GPU[GPU 0<br/>KV Cache]
        P_NIXL[NIXL Sender<br/>åºåˆ—åŒ–]
        P_NIC[RDMA NIC]
    end
    
    subgraph Network [ç½‘ç»œå±‚]
        UCCL[UCCL ä¼ è¾“<br/>æ‹¥å¡æ§åˆ¶]
    end
    
    subgraph Decode [Decode èŠ‚ç‚¹]
        D_NIC[RDMA NIC]
        D_NIXL[NIXL Receiver<br/>ååºåˆ—åŒ–]
        D_GPU[GPU 0<br/>KV Cache]
    end
    
    P_GPU -->|Deviceâ†’Host| P_NIXL
    P_NIXL --> P_NIC
    P_NIC -->|RDMA Write| UCCL
    UCCL --> D_NIC
    D_NIC --> D_NIXL
    D_NIXL -->|Hostâ†’Device| D_GPU
    
    style Prefill fill:#e1f5fe
    style Network fill:#e8f5e9
    style Decode fill:#fff3e0
```

#### UCCL ä¸»æœºç«¯æ‹¥å¡æ§åˆ¶

**ä¸ºä»€ä¹ˆä¸ä¾èµ–ç¡¬ä»¶å¸è½½?**

ä¼ ç»Ÿ RDMA ä¾èµ– NIC ç¡¬ä»¶å¤„ç†æ‹¥å¡:
- âœ… å»¶è¿Ÿä½ (æ—  CPU å‚ä¸)
- âŒ æ‹¥å¡æ§åˆ¶ç­–ç•¥å›ºå®š,æ— æ³•é€‚åº” LLM æµé‡ç‰¹å¾
- âŒ å¤šæµç«äº‰æ—¶å…¬å¹³æ€§å·®

**UCCL çš„ä¸»æœºç«¯æ–¹æ¡ˆ**:

```python
# ä¼ªä»£ç 
class UCCLTransport:
    def send_kv_blocks(self, blocks, dest):
        # æµåˆ†å‰² (é¿å…å¤§å—é˜»å¡)
        chunks = split_into_chunks(blocks, chunk_size=1MB)
        
        for chunk in chunks:
            # åŠ¨æ€æ‹¥å¡çª—å£
            while self.congestion_window_full():
                time.sleep(microseconds=10)
            
            # å‘é€æ•°æ®
            rdma_write(chunk, dest)
            
            # æ ¹æ® ACK è°ƒæ•´çª—å£
            self.adjust_window()
```

**å®æµ‹æ•ˆæœ** (4GB KV ä¼ è¾“):
- åŸºçº¿ UCX: 362ms â†’ æ‹¥å¡å 424ms (+17.1%)
- llm-d UCCL: 359ms â†’ æ‹¥å¡å 384ms (+7.1%)
- **å¼¹æ€§ä¼˜åŠ¿**: 2.4x æ›´å¼ºçš„æŠ—æ‹¥å¡èƒ½åŠ›

---

### 2.5 Autoscaler é¥±å’Œåº¦æ„ŸçŸ¥ç®—æ³•

#### æ ¸å¿ƒæŒ‡æ ‡

```python
# é¥±å’Œåº¦è®¡ç®—
saturation = (
    kv_memory_utilization * 0.5 +   # KV Cache å ç”¨
    queue_depth_ratio * 0.3 +        # é˜Ÿåˆ—æ·±åº¦
    throughput_degradation * 0.2     # ååä¸‹é™æ¯”ä¾‹
)

if saturation > 0.8:
    scale_up()
elif saturation < 0.3:
    scale_down()
```

#### ä¸ HPA é›†æˆ

```mermaid
flowchart LR
    WVA[WVA Controller] -->|è®¡ç®—æœŸæœ›å‰¯æœ¬æ•°| PM[Prometheus Metrics]
    PM -->|wva_desired_replicas| HPA[Horizontal Pod Autoscaler]
    HPA -->|è°ƒæ•´å‰¯æœ¬| Deploy[Deployment]
    Deploy -.åé¦ˆ.-> WVA
    
    style WVA fill:#e1f5fe
    style HPA fill:#fff9c4
```

**é…ç½®ç¤ºä¾‹**:
```yaml
apiVersion: llmd.ai/v1alpha1
kind: VariantAutoscaling
metadata:
  name: llama-autoscaler
spec:
  scaleTargetRef:
    kind: Deployment
    name: llama-70b
  modelID: "meta/llama-3.1-70b"
  saturationThreshold: 0.8  # 80% é¥±å’Œåº¦è§¦å‘æ‰©å®¹
```

---

### âœ… èºæ—‹ 2 éªŒæ”¶æ ‡å‡†

å®Œæˆæœ¬å±‚å­¦ä¹ å,ä½ åº”è¯¥èƒ½å¤Ÿ:

- [ ] ç”»å‡ºä»è¯·æ±‚åˆ°å“åº”çš„å®Œæ•´æ—¶åºå›¾ (9 ä¸ªå…³é”®æ­¥éª¤)
- [ ] è§£é‡Šè°ƒåº¦ç®—æ³•çš„ä¸‰é˜¶æ®µ: Filter (æ’é™¤) â†’ Score (è¯„åˆ†) â†’ Select (é€‰æ‹©)
- [ ] è¯´æ˜ KV Cache ä¸‰çº§å­˜å‚¨çš„å¸è½½/åŠ è½½æ—¶æœº
- [ ] ç†è§£ UCCL ä¸»æœºç«¯æ‹¥å¡æ§åˆ¶ç›¸æ¯”ç¡¬ä»¶å¸è½½çš„ä¼˜åŠ¿
- [ ] è®¡ç®—ç»™å®šåœºæ™¯ä¸‹çš„é¥±å’Œåº¦æŒ‡æ ‡

---

### ğŸ”— ä¸‹ä¸€æ­¥

æŒæ¡äº†åº•å±‚æœºåˆ¶å,ä¸‹ä¸€å±‚æˆ‘ä»¬å°†è¿›å…¥ **ç”Ÿäº§éƒ¨ç½²ä¸è¿ç»´å®æˆ˜**,å­¦ä¹ ä¸‰æ¡ Well-Lit Paths çš„é€‰å‹ã€é…ç½®è°ƒä¼˜ä¸æ•…éšœæ’æŸ¥ã€‚

---

## ğŸŒ€ èºæ—‹ 3: å®æˆ˜å±‚ - ç”Ÿäº§éƒ¨ç½²ä¸è¿ç»´æƒè¡¡

### æœ¬å±‚ç›®æ ‡
æŒæ¡ llm-d çš„ä¸‰æ¡ Well-Lit Paths é€‰å‹ã€æ ¸å¿ƒé…ç½®å‚æ•°ã€ç›‘æ§æŒ‡æ ‡ä½“ç³»ä¸å…¸å‹æ•…éšœæ’æŸ¥æ–¹æ³•ã€‚

---

### 3.1 ä¸‰æ¡ Well-Lit Paths å¯¹æ¯”ä¸é€‰å‹

llm-d æä¾›ä¸‰æ¡ç»è¿‡ç”Ÿäº§éªŒè¯çš„éƒ¨ç½²è·¯å¾„,æ¯æ¡è·¯å¾„é’ˆå¯¹ä¸åŒå·¥ä½œè´Ÿè½½ä¼˜åŒ–:

| ç»´åº¦ | Inference Scheduling | P/D Disaggregation | Wide-EP |
|------|---------------------|-------------------|---------|
| **æ ¸å¿ƒä¼˜åŒ–** | æ™ºèƒ½è°ƒåº¦ + ç¼“å­˜å¤ç”¨ | è®¡ç®—/å†…å­˜è§£è€¦ | æ‰¹å¤„ç†åå |
| **é€‚ç”¨æ¨¡å‹** | 7B-70B | 70B-175B | MoE (DeepSeek/Mixtral) |
| **å·¥ä½œè´Ÿè½½** | å¤šè½®å¯¹è¯ã€RAGã€Agent | é•¿ä¸Šä¸‹æ–‡ (10k+ input) | æ‰¹å¤„ç†ã€ç¦»çº¿æ¨ç† |
| **Prefix å¤ç”¨** | **é«˜** (>50%) | ä¸­ (20-50%) | ä½ (<20%) |
| **ç½‘ç»œè¦æ±‚** | æ•°æ®ä¸­å¿ƒç½‘ç»œ | **RDMA/IB** (å¿…éœ€) | **RDMA + NVLink** |
| **æˆæœ¬** | ğŸ’° (æœ€ä½) | ğŸ’°ğŸ’° | ğŸ’°ğŸ’°ğŸ’° (æœ€é«˜) |
| **å»¶è¿Ÿ** | **TTFT æœ€ä¼˜** (50-150ms) | TTFT ä¸­ç­‰ (300-500ms) | TTFT é«˜ (>1s) |
| **åå** | ä¸­ (10-15k tok/s) | ä¸­ (20-40k tok/s) | **æœ€é«˜** (50k+ tok/s) |

#### å†³ç­–æ ‘

```mermaid
flowchart TD
    Start{æ¨¡å‹å‚æ•°è§„æ¨¡?}
    Start -->|7B-70B| Q1{Prefix å¤ç”¨ç‡?}
    Start -->|70B-175B| Q2{è¾“å…¥é•¿åº¦?}
    Start -->|MoE æ¶æ„| Path3[Wide-EP]
    
    Q1 -->|>50%| Path1[Inference Scheduling]
    Q1 -->|<50%| Fallback1[æ ‡å‡† vLLM + HPA]
    
    Q2 -->|>10k tokens| Path2[P/D Disaggregation]
    Q2 -->|<10k tokens| Q3{æœ‰ RDMA?}
    Q3 -->|æ˜¯| Path2
    Q3 -->|å¦| Path1
    
    style Path1 fill:#e1f5fe
    style Path2 fill:#fff3e0
    style Path3 fill:#e8f5e9
    style Fallback1 fill:#ffebee
```

---

### 3.2 æ ¸å¿ƒé…ç½®å‚æ•°ä¸è°ƒä¼˜

#### è·¯å¾„ 1: Inference Scheduling

**éƒ¨ç½²æ‹“æ‰‘**:
```yaml
# vLLM Deployment
replicas: 8
resources:
  nvidia.com/gpu: 2  # TP=2
env:
  - name: VLLM_ENABLE_PREFIX_CACHING
    value: "true"
  - name: VLLM_GPU_MEMORY_UTILIZATION
    value: "0.90"  # é¢„ç•™ 10% ç»™ KV Cache

# Inference Scheduler
scorers:
  - type: prefix-aware
    weight: 100
    parameters:
      hashBlockSize: 5  # Hash å—å¤§å° (å½±å“ç¼“å­˜ç²’åº¦)
  - type: load-aware
    weight: 50
```

**å…³é”®å‚æ•°è°ƒä¼˜**:

| å‚æ•° | é»˜è®¤å€¼ | è°ƒä¼˜å»ºè®® |
|------|--------|---------|
| `hashBlockSize` | 5 | Prefix è¶Šé•¿è®¾ç½®è¶Šå¤§ (10-20),è¶ŠçŸ­è¶Šå° (3-5) |
| `gpu_memory_utilization` | 0.90 | KV Cache å¯†é›†åœºæ™¯é™åˆ° 0.85 |
| `max_num_seqs` | 256 | é«˜ QPS åœºæ™¯æå‡åˆ° 512 |
| `scorer.weight` | 50/50 | é«˜ Prefix å¤ç”¨æ—¶ prefix-aware æƒé‡ 100 |

**ç›‘æ§æŒ‡æ ‡**:
```promql
# ç¼“å­˜å‘½ä¸­ç‡
rate(vllm_cache_hit_total[5m]) / rate(vllm_cache_lookup_total[5m])

# é˜Ÿåˆ—ç­‰å¾…æ—¶é—´
histogram_quantile(0.95, vllm_queue_wait_seconds)

# TTFT P95
histogram_quantile(0.95, vllm_time_to_first_token_seconds)
```

---

#### è·¯å¾„ 2: P/D Disaggregation

**éƒ¨ç½²æ‹“æ‰‘**:
```yaml
# Prefill Pods
replicas: 4
tensorParallel: 1  # TP=1,è®¡ç®—ä¼˜åŒ–
env:
  - name: VLLM_DISAGG_MODE
    value: "prefill"

# Decode Pods
replicas: 1
tensorParallel: 4  # TP=4,å¸¦å®½ä¼˜åŒ–
env:
  - name: VLLM_DISAGG_MODE
    value: "decode"

# NIXL ç½‘ç»œé…ç½®
nixl:
  backend: "uccl"  # æˆ– "ucx"
  transport: "rdma"  # RDMA > TCP-X
```

**xPyD æ¯”ä¾‹è°ƒä¼˜**:

| ISL/OSL æ¯”ä¾‹ | æ¨è Prefill:Decode | ç†ç”± |
|-------------|-------------------|------|
| 10:1 (10k/1k) | 8:1 | Prefill å‹åŠ›å¤§ |
| 5:1 (5k/1k) | 4:1 | å¹³è¡¡é…ç½® |
| 1:1 (1k/1k) | 2:1 | Decode ä¸»å¯¼ |

**ç½‘ç»œå¸¦å®½è§„åˆ’**:
```python
# KV Cache å¤§å°ä¼°ç®—
kv_size_per_token = (
    num_layers * 2 *           # K + V
    hidden_dim *               # éšè—å±‚ç»´åº¦
    bytes_per_param            # FP16 = 2 bytes
)

# Llama-3.1-70B ç¤ºä¾‹
kv_size = 80 * 2 * 8192 * 2 = 2.56 MB/token

# 10k token ä¼ è¾“éœ€è¦
transfer_time = (10000 * 2.56 MB) / (100 Gbps / 8) 
              = 2.05 seconds  # éœ€è¦ RDMA ä¼˜åŒ–!
```

**æ•…éšœæ’æŸ¥**:
```bash
# æ£€æŸ¥ RDMA è¿æ¥
ibv_devices  # ç¡®è®¤ IB è®¾å¤‡
ibv_devinfo mlx5_0 | grep state  # PORT_ACTIVE

# KV ä¼ è¾“å»¶è¿Ÿç›‘æ§
kubectl logs decode-pod | grep "KV transfer latency"
# æœŸæœ›å€¼: <500ms (RDMA), <2s (TCP-X)
```

---

#### è·¯å¾„ 3: Wide-EP

**éƒ¨ç½²æ‹“æ‰‘**:
```yaml
# DeepSeek-R1 ç¤ºä¾‹
prefill:
  replicas: 16
  expertParallel: 16  # EP=16
  tensorParallel: 1

decode:
  replicas: 16
  expertParallel: 16
  tensorParallel: 1

# LeaderWorkerSet é…ç½®
leaderWorkerSet:
  enabled: true
  size: 16  # EP å¹¶è¡Œåº¦
```

**æ€§èƒ½æŒ‡æ ‡** (DeepSeek-R1, NVIDIA B200):
- æ€»åå: **~50k output tokens/sec**
- å• GPU åå: **~3.1k output tokens/sec**
- P95 å»¶è¿Ÿ: <2s (æ‰¹å¤„ç†åœºæ™¯å¯æ¥å—)

---

### 3.3 ç›‘æ§æŒ‡æ ‡ä½“ç³»

#### å››å±‚ç›‘æ§é‡‘å­—å¡”

```mermaid
flowchart TB
    subgraph L1 [L1: ä¸šåŠ¡æŒ‡æ ‡ - SLI/SLO]
        B1[TTFT P95 < 200ms]
        B2[åå > 10k tok/s]
        B3[æˆåŠŸç‡ > 99.9%]
    end
    
    subgraph L2 [L2: åº”ç”¨æŒ‡æ ‡ - vLLM]
        A1[é˜Ÿåˆ—æ·±åº¦]
        A2[KV åˆ©ç”¨ç‡]
        A3[æ‰¹å¤„ç†å¤§å°]
    end
    
    subgraph L3 [L3: èµ„æºæŒ‡æ ‡ - GPU/ç½‘ç»œ]
        R1[GPU åˆ©ç”¨ç‡]
        R2[HBM ä½¿ç”¨]
        R3[ç½‘ç»œå¸¦å®½]
    end
    
    subgraph L4 [L4: åŸºç¡€è®¾æ–½ - K8s]
        I1[Pod å¥åº·çŠ¶æ€]
        I2[èŠ‚ç‚¹èµ„æº]
    end
    
    B1 --> A1
    B2 --> A2
    A1 --> R1
    A2 --> R2
    R1 --> I1
    
    style L1 fill:#e1f5fe
    style L2 fill:#fff9c4
    style L3 fill:#fff3e0
    style L4 fill:#f3e5f5
```

#### å…³é”®å‘Šè­¦è§„åˆ™

```yaml
# Prometheus AlertManager è§„åˆ™
groups:
  - name: llm-d-slo
    rules:
      # å‘Šè­¦ 1: TTFT è¶… SLO
      - alert: HighTTFT
        expr: |
          histogram_quantile(0.95, 
            rate(vllm_time_to_first_token_seconds_bucket[5m])
          ) > 0.2
        for: 5m
        annotations:
          summary: "TTFT P95 è¶…è¿‡ 200ms"
          
      # å‘Šè­¦ 2: KV Cache å†…å­˜å‹åŠ›
      - alert: KVMemoryPressure
        expr: |
          vllm_kv_cache_utilization > 0.95
        for: 3m
        annotations:
          summary: "KV Cache ä½¿ç”¨ç‡è¶… 95%,å³å°† OOM"
          
      # å‘Šè­¦ 3: é˜Ÿåˆ—å †ç§¯
      - alert: QueueBacklog
        expr: |
          vllm_queue_depth > 100
        for: 5m
        annotations:
          summary: "è¯·æ±‚é˜Ÿåˆ—å †ç§¯è¶… 100"
```

---

### 3.4 å…¸å‹æ•…éšœå†³ç­–æ ‘

#### é—®é¢˜ 1: TTFT çªç„¶å‡é«˜

```mermaid
flowchart TD
    Start[TTFT P95 > SLO] --> Q1{ç¼“å­˜å‘½ä¸­ç‡ä¸‹é™?}
    Q1 -->|æ˜¯| Fix1[æ£€æŸ¥ Scheduler é…ç½®<br/>å¢åŠ  prefix-aware æƒé‡]
    Q1 -->|å¦| Q2{é˜Ÿåˆ—æ·±åº¦å¢åŠ ?}
    
    Q2 -->|æ˜¯| Fix2[è§¦å‘æ‰©å®¹<br/>æˆ–é™ä½ max_num_seqs]
    Q2 -->|å¦| Q3{GPU åˆ©ç”¨ç‡ä½?}
    
    Q3 -->|æ˜¯| Fix3[æ£€æŸ¥ç½‘ç»œå»¶è¿Ÿ<br/>P/D ä¼ è¾“ç“¶é¢ˆ?]
    Q3 -->|å¦| Fix4[æ£€æŸ¥æ¨¡å‹çƒ­ç‚¹<br/>å• Pod è¿‡è½½?]
    
    style Fix1 fill:#e8f5e9
    style Fix2 fill:#e8f5e9
    style Fix3 fill:#fff3e0
    style Fix4 fill:#ffebee
```

**è°ƒè¯•å‘½ä»¤**:
```bash
# 1. æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡
kubectl exec -it vllm-pod -- curl localhost:8000/metrics | grep cache_hit

# 2. æ£€æŸ¥é˜Ÿåˆ—æ·±åº¦
kubectl top pod vllm-pod --containers | grep queue_depth

# 3. æ£€æŸ¥ P/D ä¼ è¾“å»¶è¿Ÿ
kubectl logs decode-pod | grep "transfer_latency_ms"
```

---

#### é—®é¢˜ 2: ååä¸‹é™

| ç—‡çŠ¶ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|---------|---------|
| GPU åˆ©ç”¨ç‡ <50% | æ‰¹å¤„ç†å¤§å°ä¸è¶³ | å¢åŠ  `max_num_batched_tokens` |
| KV åˆ©ç”¨ç‡ >95% | å†…å­˜å‹åŠ›å¯¼è‡´é©±é€ | å¯ç”¨ CPU offloading |
| ç½‘ç»œå¸¦å®½æ‰“æ»¡ | P/D ä¼ è¾“ç«äº‰ | ä¼˜åŒ– xPyD æ¯”ä¾‹æˆ–å¢åŠ å¸¦å®½ |
| é˜Ÿåˆ—æ·±åº¦ >200 | è´Ÿè½½è¶…å®¹é‡ | è§¦å‘æ‰©å®¹æˆ–é™æµ |

---

#### é—®é¢˜ 3: OOM (Out of Memory)

**æ ¹å› åˆ†æ**:
```python
# KV Cache å†…å­˜ä¼°ç®—
total_kv_memory = (
    max_num_seqs *           # æœ€å¤§å¹¶å‘åºåˆ—
    max_seq_len *            # æœ€å¤§åºåˆ—é•¿åº¦
    kv_size_per_token        # å• Token KV å¤§å°
)

# Llama-70B, max_num_seqs=256, max_seq_len=8192
total_kv = 256 * 8192 * 2.56 MB = 5.4 GB  # éœ€è¦é¢„ç•™!
```

**é¢„é˜²æªæ–½**:
1. **åŠ¨æ€å†…å­˜é¢„ç•™**:
   ```yaml
   env:
     - name: VLLM_GPU_MEMORY_UTILIZATION
       value: "0.85"  # é¢„ç•™ 15% ç»™ KV Cache
   ```

2. **å¯ç”¨ KV åˆ†å±‚å¸è½½**:
   ```yaml
   env:
     - name: VLLM_KV_CACHE_OFFLOAD
       value: "cpu"  # æˆ– "filesystem"
   ```

3. **é™åˆ¶å¹¶å‘æ•°**:
   ```yaml
   env:
     - name: VLLM_MAX_NUM_SEQS
       value: "128"  # æ ¹æ®å®æµ‹è°ƒæ•´
   ```

---

### 3.5 æˆæœ¬ä¼˜åŒ–æƒè¡¡çŸ©é˜µ

| ä¼˜åŒ–æ–¹å‘ | æ€§èƒ½å½±å“ | æˆæœ¬é™ä½ | é€‚ç”¨åœºæ™¯ |
|---------|---------|---------|---------|
| **å¯ç”¨ Prefix Caching** | ğŸš€ TTFT -90% | ğŸ’° GPU æ•°é‡ -30% | é«˜ Prefix å¤ç”¨ |
| **CPU KV Offloading** | ğŸ¢ TTFT +20% | ğŸ’°ğŸ’° å¹¶å‘ +10x | ä½é¢‘è®¿é—®åœºæ™¯ |
| **P/D åˆ†ç¦» (TP=1 Prefill)** | ğŸš€ åå +50% | ğŸ’° å‡å°‘é«˜ç«¯ GPU | é•¿ä¸Šä¸‹æ–‡ |
| **Scale-to-Zero** | â¸ï¸ å†·å¯åŠ¨ 30s | ğŸ’°ğŸ’°ğŸ’° ç©ºé—²æˆæœ¬ -100% | é—´æ­‡æ€§å·¥ä½œè´Ÿè½½ |
| **Spot å®ä¾‹** | âš ï¸ å¯ç”¨æ€§ -10% | ğŸ’°ğŸ’° æˆæœ¬ -70% | å®¹é”™æ‰¹å¤„ç† |

**æœ€ä½³å®è·µ**:
- **äº¤äº’å¼æœåŠ¡**: Inference Scheduling + Prefix Caching + æŒ‰éœ€æ‰©å®¹
- **æ‰¹å¤„ç†ä»»åŠ¡**: Wide-EP + Spot å®ä¾‹ + Scale-to-Zero
- **æ··åˆå·¥ä½œè´Ÿè½½**: P/D åˆ†ç¦» + åˆ†å±‚ç¼“å­˜ + å¤šä¼˜å…ˆçº§é˜Ÿåˆ—

---

### âœ… èºæ—‹ 3 éªŒæ”¶æ ‡å‡†

å®Œæˆæœ¬å±‚å­¦ä¹ å,ä½ åº”è¯¥èƒ½å¤Ÿ:

- [ ] æ ¹æ®å·¥ä½œè´Ÿè½½ç‰¹å¾é€‰æ‹©åˆé€‚çš„ Well-Lit Path (å†³ç­–æ ‘)
- [ ] é…ç½®æ ¸å¿ƒå‚æ•°: `hashBlockSize`, `xPyD æ¯”ä¾‹`, `gpu_memory_utilization`
- [ ] å»ºç«‹å››å±‚ç›‘æ§ä½“ç³»: ä¸šåŠ¡ SLI â†’ åº”ç”¨æŒ‡æ ‡ â†’ èµ„æºæŒ‡æ ‡ â†’ åŸºç¡€è®¾æ–½
- [ ] ä½¿ç”¨æ•…éšœå†³ç­–æ ‘è¯Šæ–­ TTFT å‡é«˜ã€ååä¸‹é™ã€OOM ç­‰å…¸å‹é—®é¢˜
- [ ] è¯„ä¼°æˆæœ¬ä¼˜åŒ–æ–¹æ¡ˆçš„æ€§èƒ½å½±å“ä¸é€‚ç”¨åœºæ™¯

---

### ğŸ“ æ€»ç»“

**llm-d çš„æ ¸å¿ƒä»·å€¼**:
1. **æ€§èƒ½**: é€šè¿‡æ™ºèƒ½è°ƒåº¦ã€åˆ†å±‚ç¼“å­˜ã€P/D åˆ†ç¦»å®ç° SOTA æ¨ç†æ€§èƒ½
2. **è¿ç»´**: Kubernetes åŸç”Ÿ,æ— ç¼é›†æˆ HPA/Prometheus/Istio
3. **çµæ´»**: ä¸‰æ¡ Well-Lit Paths è¦†ç›– 90% ç”Ÿäº§åœºæ™¯
4. **æˆæœ¬**: Prefix Caching + Autoscaling å¯é™ä½ 50%+ GPU æˆæœ¬

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**:
- ğŸ“– æ·±å…¥é˜…è¯»å„ç»„ä»¶ä¸“é¢˜æ–‡æ¡£ (è§ä¸‹æ–¹é“¾æ¥)
- ğŸ§ª åœ¨æµ‹è¯•ç¯å¢ƒéƒ¨ç½² Quickstart éªŒè¯æ¦‚å¿µ
- ğŸ“Š æ ¹æ®å®é™…å·¥ä½œè´Ÿè½½é€‰æ‹© Well-Lit Path
- ğŸš€ æ¸è¿›å¼è¿ç§»ç”Ÿäº§æµé‡å¹¶æŒç»­ä¼˜åŒ–

---

## ğŸ”— æ·±å…¥é˜…è¯»

### æ ¸å¿ƒç»„ä»¶æ·±åº¦å‰–æ

- [**Inference Scheduler (æ¨ç†è°ƒåº¦å™¨)**](./components/inference-scheduler.md) - æ™ºèƒ½è·¯ç”±å†³ç­–å¼•æ“
- [**KV Cache Management (KV ç¼“å­˜ç®¡ç†)**](./components/kv-cache.md) - åˆ†å±‚ç¼“å­˜æ¶æ„
- [**Prefill/Decode Disaggregation (P/D åˆ†ç¦»)**](./components/pd-disaggregation.md) - è®¡ç®—ä¸å†…å­˜è§£è€¦
- [**Workload Variant Autoscaler (å¼¹æ€§ä¼¸ç¼©)**](./components/autoscaler.md) - é¥±å’Œåº¦æ„ŸçŸ¥æ‰©ç¼©å®¹
- [**Resilient Networking (å¼¹æ€§ç½‘ç»œ)**](./components/networking.md) - UCCL/NIXL ä¼ è¾“ä¼˜åŒ–

### ç”Ÿäº§å®è·µ

- [**Production Patterns (ç”Ÿäº§æ¨¡å¼)**](./integration/production-patterns.md) - ä¸‰æ¡ Well-Lit Paths å¯¹æ¯”ä¸é€‰å‹

---

## ğŸ”— ç›¸å…³æŠ€æœ¯

- [**WVA (Workload Variant Autoscaler)**](../../autoscaling/wva/) - llm-d é…å¥—çš„æ™ºèƒ½å¼¹æ€§ä¼¸ç¼©å™¨
- [**GPU Operator**](../../hardware/) - NVIDIA GPU èµ„æºç®¡ç†
- [**Prometheus**](../../monitoring/) - ç›‘æ§æŒ‡æ ‡é‡‡é›†ä¸å‘Šè­¦
- [**Istio/Envoy**](../../networking/) - Service Mesh ä¸ Gateway å®ç°

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [llm-d å®˜æ–¹æ–‡æ¡£](https://llm-d.ai/)
- [llm-d GitHub ä»“åº“](https://github.com/llm-d/llm-d)
- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [Gateway API Inference Extension](https://github.com/kubernetes-sigs/gateway-api-inference-extension)
