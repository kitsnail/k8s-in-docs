# llm-d-modelservice: Kubernetes Operator for LLM Inference Service Management

> **é¡¹ç›®å®šä½**: å£°æ˜å¼ LLM æ¨ç†æœåŠ¡ç¼–æ’å¼•æ“  
> **æ ¸å¿ƒä»·å€¼**: é€šè¿‡ CRD å°è£… Prefill/Decode åˆ†ç¦»æ¶æ„ã€æ¨¡å‹åŠ è½½ã€è·¯ç”±é…ç½®çš„å®Œæ•´æ¨ç†æ ˆ  
> **é€‚ç”¨åœºæ™¯**: AI å¹³å°çš„å¤šç§Ÿæˆ·æ¨ç†æœåŠ¡ã€ä¼ä¸šçº§æ¨¡å‹æœåŠ¡åŒ–ã€å¼‚æ„ç®—åŠ›ä¼˜åŒ–

---

## ğŸŒ€ èºæ—‹ 1: æ¦‚å¿µå±‚ - ä¸ºä»€ä¹ˆéœ€è¦ Operator ç®¡ç†æ¨ç†æœåŠ¡?

### æœ¬å±‚ç›®æ ‡
å»ºç«‹å¯¹ llm-d-modelservice æ ¸å¿ƒä»·å€¼çš„ç›´è§‚è®¤çŸ¥,ç†è§£ **BaseConfig + ModelService** çš„é…ç½®åˆ†å±‚å“²å­¦,ä»¥åŠ Prefill/Decode åˆ†ç¦»æ¶æ„çš„æœ¬è´¨ã€‚

---

### å…¨æ™¯ç±»æ¯”: AI å¤–å–å¹³å°çš„è‡ªåŠ¨åŒ–ä¸­å¤®å¨æˆ¿ ğŸ±

æƒ³è±¡ä½ åœ¨è¿è¥ä¸€ä¸ªè¦†ç›–å…¨å›½çš„ AI å¤–å–å¹³å°(ç±»æ¯” AI æ¨ç†æœåŠ¡å¹³å°),éœ€è¦ç®¡ç†æ•°ç™¾å®¶é—¨åº—(ç±»æ¯”æ•°ç™¾ä¸ªæ¨¡å‹æ¨ç†å®ä¾‹)ã€‚

#### æ²¡æœ‰ llm-d-modelservice çš„ä¸–ç•Œ

æ¯ä¸Šçº¿ä¸€ä¸ªæ–°æ¨¡å‹,è¿ç»´éœ€è¦æ‰‹åŠ¨ç¼–å†™ä¸€å¥— Kubernetes YAML:
```yaml
# æ‰‹åŠ¨åˆ›å»º Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama2-7b-decode
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: vllm
        image: vllm/vllm-openai:v0.3.0
        args: ["--model", "meta-llama/Llama-2-7b-hf"]
        env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef: ...
        volumeMounts:
        - name: model-cache
          mountPath: /cache
      volumes:
      - name: model-cache
        emptyDir:
          sizeLimit: 50Gi

---
# æ‰‹åŠ¨åˆ›å»º Service
apiVersion: v1
kind: Service
metadata:
  name: llama2-7b-svc
spec:
  selector:
    app: llama2-7b-decode
  ports:
  - port: 8000

---
# æ‰‹åŠ¨åˆ›å»º HTTPRoute
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: llama2-7b-route
spec:
  parentRefs:
  - name: inference-gateway
  rules:
  - matches:
    - path:
        value: /v1/completions
    filters:
    - type: RequestHeaderModifier
      requestHeaderModifier:
        set:
        - name: X-Model
          value: meta-llama/Llama-2-7b-hf

---
# è¿˜éœ€è¦åˆ›å»º RBACã€InferencePoolã€InferenceModel...
```

**ç—›ç‚¹æ¸…å•**:
1. âŒ æ¯ä¸ªæ¨¡å‹éœ€è¦ 300+ è¡Œ YAML,é‡å¤é…ç½® 80%
2. âŒ Prefill/Decode åˆ†ç¦»éœ€è¦æ‰‹åŠ¨é…ç½®ä¸¤å¥— Deployment + è·¯ç”±ç­–ç•¥
3. âŒ æ¨¡å‹è·¯å¾„ã€Tokenã€æŒ‚è½½ç‚¹æ•£è½åœ¨å„å¤„,æ”¹ä¸€å¤„è¦æ‰¾ 5 ä¸ªåœ°æ–¹
4. âŒ å¹³å°å‡çº§(å¦‚ç»Ÿä¸€é•œåƒç‰ˆæœ¬)éœ€è¦é€ä¸ªä¿®æ”¹æ¯ä¸ªæ¨¡å‹çš„é…ç½®
5. âŒ æ–°äººä¸Šæ‰‹éœ€è¦ç†è§£ K8sã€vLLMã€Gateway API ä¸‰å±‚æ¦‚å¿µ

**å¤–å–å¹³å°ç±»æ¯”**: æ¯å¼€ä¸€å®¶æ–°åº—,éƒ½è¦ä»å¤´è®¾è®¡èœå•ã€è£…ä¿®é£æ ¼ã€é‡‡è´­æµç¨‹ã€é…é€è·¯çº¿â€”â€”å³ä½¿ 90% çš„åº—éƒ½æ˜¯"éº»è¾£çƒ«"ã€‚

---

#### llm-d-modelservice çš„è§£å†³æ–¹æ¡ˆ

**æ ¸å¿ƒæ€æƒ³**: **å¹³å°æ ‡å‡†èœè°±(BaseConfig) + å•åº—å®šåˆ¶(ModelService)**

```yaml
# ğŸ½ï¸ Step 1: å¹³å°å®šä¹‰æ ‡å‡†èœè°±(BaseConfig - ç”±å¹³å°å›¢é˜Ÿç»´æŠ¤)
apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-standard-config  # ç±»æ¯”: è¿é”å“ç‰Œçš„"æ ‡å‡†æ“ä½œæ‰‹å†Œ"
data:
  decodeDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          containers:
          - name: vllm
            image: ghcr.io/llm-d/llm-d:0.0.8
            command: ["vllm", "serve"]
            args:
            - "--port"
            - "{{ "app_port" | getPort }}"  # æ¨¡æ¿å˜é‡,è¿è¡Œæ—¶å¡«å……
            env:
            - name: HF_HOME
              value: /cache
            resources:
              limits:
                nvidia.com/gpu: 1

---
# ğŸª Step 2: æ¨¡å‹å›¢é˜Ÿåªéœ€å£°æ˜"æˆ‘è¦å¼€åº—"(ModelService)
apiVersion: llm-d.ai/v1alpha1
kind: ModelService
metadata:
  name: llama2-7b
spec:
  baseConfigMapRef:
    name: vllm-standard-config  # å¼•ç”¨æ ‡å‡†èœè°±
  
  routing:
    modelName: meta-llama/Llama-2-7b-hf  # å®¢æˆ·çœ‹åˆ°çš„èœå•å
    ports:
    - name: app_port
      port: 8000
  
  modelArtifacts:
    uri: hf://meta-llama/Llama-2-7b-hf  # é£Ÿæé‡‡è´­åœ°å€
  
  decode:
    replicas: 3  # å¼€ 3 ä¸ªçª—å£
```

**ç¥å¥‡ä¹‹å¤„**: Controller è‡ªåŠ¨åˆ›å»ºä»¥ä¸‹èµ„æº(å°±åƒä¸­å¤®å¨æˆ¿çš„è‡ªåŠ¨åŒ–ç³»ç»Ÿ):
- âœ… Decode Deployment(å¸¦æ¨¡æ¿å˜é‡å¡«å……)
- âœ… Decode Service
- âœ… HTTPRoute(è‡ªåŠ¨è·¯ç”±åˆ°æ­£ç¡®çš„æ¨¡å‹)
- âœ… InferencePool + InferenceModel(Gateway API æ‰©å±•)
- âœ… ServiceAccount + RoleBinding(RBAC æƒé™)
- âœ… ConfigMap(ç¯å¢ƒå˜é‡æ³¨å…¥)

---

### ä¸ºä»€ä¹ˆéœ€è¦ BaseConfig + ModelService ä¸¤å±‚?

**å¤–å–å¹³å°ç±»æ¯”**:

| å±‚çº§ | å¤–å–å¹³å° | llm-d-modelservice | ç®¡ç†è€… |
|------|---------|-------------------|--------|
| **å¹³å°å±‚** | è¿é”å“ç‰Œæ ‡å‡†(è£…ä¿®é£æ ¼ã€ç»Ÿä¸€é‡‡è´­ã€ç‰©æµç³»ç»Ÿ) | **BaseConfig**: æ ‡å‡†åŒ–å®¹å™¨é•œåƒã€èµ„æºé™åˆ¶ã€æŒ‚è½½ç­–ç•¥ | å¹³å° SRE å›¢é˜Ÿ |
| **é—¨åº—å±‚** | å•åº—ç‰¹è‰²(èœå•è°ƒæ•´ã€ä¼˜æƒ æ´»åŠ¨ã€è¥ä¸šæ—¶é—´) | **ModelService**: æ¨¡å‹è·¯å¾„ã€å‰¯æœ¬æ•°ã€GPU ç±»å‹ | æ¨¡å‹å¼€å‘å›¢é˜Ÿ |

**çœŸå®åœºæ™¯**:
- ğŸ”§ **å¹³å°å‡çº§**: vLLM ä» v0.3.0 â†’ v0.4.0,åªéœ€ä¿®æ”¹ BaseConfig çš„ `image` å­—æ®µ,æ‰€æœ‰å¼•ç”¨å®ƒçš„ ModelService è‡ªåŠ¨ç»§æ‰¿æ–°ç‰ˆæœ¬
- ğŸ¯ **ä¸ªæ€§åŒ–é…ç½®**: Llama2-70B éœ€è¦ 8 å¼  A100,åªéœ€åœ¨ ModelService è¦†ç›– `resources.limits`
- ğŸ”’ **å®‰å…¨éš”ç¦»**: ä¸åŒç§Ÿæˆ·ä½¿ç”¨ä¸åŒ BaseConfig,äº’ä¸å½±å“

---

### Prefill/Decode åˆ†ç¦»: å‰å¨ä¸å‡ºé¤çª—å£çš„åˆ†å·¥

**ä¼ ç»Ÿå•ä½“æ¨ç†çš„é—®é¢˜(å¤–å–å¹³å°ç±»æ¯”)**:

å‡è®¾ä¸€ä¸ªå¨å¸ˆæ—¢è¦å¤‡èœ(Prefill - å¤„ç† prompt)ã€åˆè¦ç‚’èœå‡ºé¤(Decode - ç”Ÿæˆ token):
- å½“æ¥äº†ä¸€ä¸ªå¤§å•(é•¿ prompt),å¨å¸ˆå¿™ç€åˆ‡èœ 10 åˆ†é’Ÿ,å¯¼è‡´åé¢ 20 ä¸ªå°å•(çŸ­ prompt)å…¨éƒ¨å †ç§¯
- GPU åˆ©ç”¨ç‡ä¸å‡: å¤‡èœéœ€è¦å¤§é‡å†…å­˜(KV Cache),å‡ºé¤éœ€è¦é«˜ç®—åŠ›(ç”Ÿæˆé€Ÿåº¦)

**llm-d-modelservice çš„è§£å†³æ–¹æ¡ˆ**:

```yaml
spec:
  # å‰å¨ - ä¸“é—¨è´Ÿè´£å¤‡èœ(Prefill)
  prefill:
    replicas: 2  # 2 ä¸ªå¤‡èœå¸ˆå‚…
    parallelism:
      tensor: 4  # æ¯ä¸ªå¸ˆå‚…æœ‰ 4 æŠŠåˆ€(TP=4)
    acceleratorTypes:
      labelKey: nvidia.com/gpu.product
      labelValues: ["A100-80GB"]  # å¤‡èœéœ€è¦å¤§å†…å­˜
  
  # å‡ºé¤çª—å£ - ä¸“é—¨è´Ÿè´£ç‚’èœå‡ºé¤(Decode)
  decode:
    replicas: 5  # 5 ä¸ªç‚’èœç¶å°
    parallelism:
      tensor: 1  # å•ç¶å¿«é€Ÿå‡ºé¤(TP=1)
    acceleratorTypes:
      labelValues: ["A10G"]  # å‡ºé¤åªéœ€è¦ç®—åŠ›,ä¸éœ€è¦å¤§æ˜¾å­˜
```

**æŠ€æœ¯æ”¶ç›Š**:
- ğŸ’° **æˆæœ¬ä¼˜åŒ–**: Prefill ç”¨è´µçš„ A100(å†…å­˜å¤§),Decode ç”¨ä¾¿å®œçš„ A10G(ç®—åŠ›å¤Ÿ)
- ğŸ“ˆ **ååæå‡**: é•¿çŸ­è¯·æ±‚åˆ†æµ,é¿å…é˜Ÿå¤´é˜»å¡(Head-of-Line Blocking)
- ğŸ”§ **ç‹¬ç«‹æ‰©ç¼©å®¹**: Prefill æ ¹æ®é˜Ÿåˆ—é•¿åº¦æ‰©å®¹,Decode æ ¹æ® token ç”Ÿæˆé€Ÿåº¦æ‰©å®¹

---

### æ¶æ„å…¨æ™¯å›¾

```mermaid
flowchart LR
    subgraph "ğŸ—ï¸ å¹³å°å±‚(ç”± SRE ç®¡ç†)"
        BC1["BaseConfig<br/>vllm-standard"]
        BC2["BaseConfig<br/>tgi-optimized"]
    end
    
    subgraph "ğŸª æ¨¡å‹å±‚(ç”±ç®—æ³•å›¢é˜Ÿç®¡ç†)"
        MS1["ModelService<br/>llama2-7b"]
        MS2["ModelService<br/>llama2-70b"]
        MS3["ModelService<br/>qwen-14b"]
    end
    
    subgraph "âš™ï¸ è‡ªåŠ¨åŒ–ç¼–æ’(Controller)"
        CTRL["ModelService<br/>Reconciliation Loop"]
    end
    
    subgraph "ğŸ¯ è¿è¡Œæ—¶èµ„æº"
        DEPLOY["Prefill/Decode<br/>Deployments"]
        SVC["Services"]
        ROUTE["HTTPRoute"]
        POOL["InferencePool"]
        MODEL["InferenceModel"]
    end
    
    BC1 -.å¼•ç”¨.-> MS1
    BC1 -.å¼•ç”¨.-> MS2
    BC2 -.å¼•ç”¨.-> MS3
    
    MS1 -->|ç›‘å¬å˜æ›´| CTRL
    MS2 -->|ç›‘å¬å˜æ›´| CTRL
    MS3 -->|ç›‘å¬å˜æ›´| CTRL
    
    CTRL -->|åˆ›å»º/æ›´æ–°| DEPLOY
    CTRL -->|åˆ›å»º/æ›´æ–°| SVC
    CTRL -->|åˆ›å»º/æ›´æ–°| ROUTE
    CTRL -->|åˆ›å»º/æ›´æ–°| POOL
    CTRL -->|åˆ›å»º/æ›´æ–°| MODEL
    
    style BC1 fill:#e1f5fe
    style BC2 fill:#e1f5fe
    style MS1 fill:#fff3e0
    style MS2 fill:#fff3e0
    style MS3 fill:#fff3e0
    style CTRL fill:#fff9c4
    style DEPLOY fill:#e8f5e9
    style SVC fill:#e8f5e9
    style ROUTE fill:#e8f5e9
    style POOL fill:#e8f5e9
    style MODEL fill:#e8f5e9
```

---

### âœ… èºæ—‹ 1 éªŒæ”¶æ ‡å‡†

å®Œæˆæœ¬å±‚å­¦ä¹ å,ä½ åº”è¯¥èƒ½å¤Ÿ:

1. **ä¸€å¥è¯å¤è¿°æ ¸å¿ƒä»·å€¼**:  
   "llm-d-modelservice é€šè¿‡ BaseConfig + ModelService ä¸¤å±‚æŠ½è±¡,è®©æ¨¡å‹å›¢é˜Ÿåªéœ€å£°æ˜'æˆ‘è¦ä»€ä¹ˆ',å¹³å°è‡ªåŠ¨ç¼–æ’å®Œæ•´çš„æ¨ç†æ ˆã€‚"

2. **å›ç­”è®¾è®¡é—®é¢˜**:
   - ä¸ºä»€ä¹ˆä¸ç›´æ¥å†™ Deployment YAML? â†’ é‡å¤åŠ³åŠ¨ + å¹³å°å‡çº§å›°éš¾
   - BaseConfig å’Œ ModelService è°ä¼˜å…ˆçº§é«˜? â†’ ModelService è¦†ç›– BaseConfig
   - Prefill/Decode åˆ†ç¦»é€‚åˆä»€ä¹ˆåœºæ™¯? â†’ é•¿çŸ­è¯·æ±‚æ··åˆ + æˆæœ¬æ•æ„Ÿ

3. **ç±»æ¯”è¿ç§»èƒ½åŠ›**:  
   èƒ½ç”¨"å¤–å–å¹³å°"ç±»æ¯”å‘éæŠ€æœ¯äººå‘˜è§£é‡Šä¸ºä»€ä¹ˆéœ€è¦è¿™å¥—ç³»ç»Ÿã€‚

---

### ğŸ”— ä¸‹ä¸€æ­¥æŒ‡å¼•

- **æƒ³äº†è§£åº•å±‚æœºåˆ¶?** â†’ è¿›å…¥ [èºæ—‹ 2: æ§åˆ¶å™¨åè°ƒå¾ªç¯ä¸æ¨¡æ¿ç³»ç»Ÿ](#-èºæ—‹-2-æœºåˆ¶å±‚---è‡ªåŠ¨åŒ–ç¼–æ’çš„æµæ°´çº¿)
- **æƒ³å¿«é€Ÿä¸Šæ‰‹é…ç½®?** â†’ ç›´æ¥è·³è½¬ [èºæ—‹ 3: ç”Ÿäº§çº§æ¨ç†æœåŠ¡é…ç½®](#-èºæ—‹-3-å®æˆ˜å±‚---ç”Ÿäº§çº§æ¨ç†æœåŠ¡é…ç½®)
- **æƒ³æ·±å…¥ CRD è®¾è®¡?** â†’ é˜…è¯» [CRD è®¾è®¡å“²å­¦](./components/crd-design.md)

---

## ğŸŒ€ èºæ—‹ 2: æœºåˆ¶å±‚ - è‡ªåŠ¨åŒ–ç¼–æ’çš„æµæ°´çº¿

### æœ¬å±‚ç›®æ ‡
æ­ç¤º llm-d-modelservice çš„æ ¸å¿ƒæœºåˆ¶:Reconciliation Loop å¦‚ä½•å·¥ä½œã€Template ç³»ç»Ÿå¦‚ä½•å¡«å……å˜é‡ã€BaseConfig å’Œ ModelService å¦‚ä½•åˆå¹¶ã€‚

---

### ğŸ’¨ è®¤çŸ¥é™å‹: ä»å¤–å–å¹³å°ç†è§£ Reconciliation Loop

**å¤–å–å¹³å°çš„è‡ªåŠ¨åŒ–ä¸­å¤®å¨æˆ¿**:

æƒ³è±¡ä¸€ä¸ªæ™ºèƒ½ä¸­æ§ç³»ç»Ÿ(ç±»æ¯” Controller),å®ƒæ¯éš”å‡ ç§’å°±ä¼šå·¡æ£€ä¸€æ¬¡:

1. **ğŸ“‹ è¯»å–è®¢å•(Watch ModelService)**:  
   "å®¢æˆ·ä¸‹å•äº† llama2-7b,è¦æ±‚ 3 ä¸ªçª—å£(replicas=3),ä½¿ç”¨ A100 GPU"

2. **ğŸ“– æŸ¥é˜…æ ‡å‡†æ‰‹å†Œ(Get BaseConfig)**:  
   "æ ‡å‡†æ‰‹å†Œè§„å®š: vLLM å®¹å™¨ç”¨ 0.0.8 ç‰ˆæœ¬,æ¯ä¸ªçª—å£é… 16 æ ¸ CPU"

3. **ğŸ”€ åˆå¹¶èœå•(Merge Logic)**:  
   - å®¢æˆ·è®¢å•ä¼˜å…ˆçº§é«˜: `replicas=3` è¦†ç›–æ‰‹å†Œçš„é»˜è®¤å€¼ `replicas=1`
   - æ‰‹å†Œå¡«è¡¥ç©ºç¼º: å®¢æˆ·æ²¡è¯´ CPU,ç”¨æ‰‹å†Œçš„ `cpu: 16`

4. **ğŸ³ ä¸‹å‘æŒ‡ä»¤(Create/Update Resources)**:  
   è‡ªåŠ¨åˆ›å»º 3 ä¸ªå¨æˆ¿(Deployment)ã€3 ä¸ªçª—å£(Service)ã€è·¯ç”±ç‰Œ(HTTPRoute)

5. **ğŸ” æŒç»­å·¡æ£€(Requeue)**:  
   æ¯æ¬¡å·¡æ£€å¯¹æ¯”"å®¢æˆ·è¦æ±‚ vs å®é™…çŠ¶æ€",å‘ç°ä¸ä¸€è‡´å°±è‡ªåŠ¨ä¿®æ­£

**æ ¸å¿ƒé€»è¾‘**: **å£°æ˜å¼(What) vs å‘½ä»¤å¼(How)**
- âŒ å‘½ä»¤å¼: "åˆ›å»º 3 ä¸ª Pod â†’ åˆ›å»º Service â†’ ç»‘å®šè·¯ç”±" (æ­¥éª¤å†™æ­»)
- âœ… å£°æ˜å¼: "æˆ‘è¦ 3 ä¸ªå‰¯æœ¬" (Controller è‡ªå·±ç®—æ€ä¹ˆåš)

---

### Controller Reconciliation Loop æ ¸å¿ƒæµç¨‹

```mermaid
sequenceDiagram
    participant User as ğŸ‘¤ ç”¨æˆ·
    participant API as K8s API Server
    participant Ctrl as ModelService<br/>Controller
    participant Tmpl as Template<br/>Engine
    participant Merge as Merge<br/>Logic
    participant K8s as Kubernetes<br/>Resources
    
    User->>API: kubectl apply -f modelservice.yaml
    API->>Ctrl: ğŸ”” Watch Event: ModelService Created
    
    rect rgb(255, 249, 196)
        Note over Ctrl: Reconciliation Loop å¼€å§‹
        Ctrl->>API: 1ï¸âƒ£ Get BaseConfig from ConfigMap
        API-->>Ctrl: baseConfig.data["decodeDeployment"]
        
        Ctrl->>Tmpl: 2ï¸âƒ£ å¡«å……æ¨¡æ¿å˜é‡
        Note over Tmpl: {{ .HFModelName }} â†’ "meta-llama/Llama-2-7b-hf"<br/>{{ "app_port" | getPort }} â†’ 8000
        Tmpl-->>Ctrl: æ¸²æŸ“åçš„ BaseConfig YAML
        
        Ctrl->>Merge: 3ï¸âƒ£ åˆå¹¶ BaseConfig + ModelService
        Note over Merge: DeepCopy + Semantic Merge<br/>ModelService ä¼˜å…ˆçº§é«˜
        Merge-->>Ctrl: æœ€ç»ˆçš„èµ„æºå®šä¹‰
        
        Ctrl->>K8s: 4ï¸âƒ£ Apply Resources
        Ctrl->>K8s: Create Decode Deployment
        Ctrl->>K8s: Create Decode Service
        Ctrl->>K8s: Create HTTPRoute
        Ctrl->>K8s: Create InferencePool/Model
        K8s-->>Ctrl: âœ… Resources Created
        
        Ctrl->>API: 5ï¸âƒ£ Update ModelService.Status
        Note over Ctrl: .status.decodeReady = "3/3"<br/>.status.decodeAvailable = 3
    end
    
    API-->>User: ModelService Ready âœ…
```

---

### æ ¸å¿ƒæœºåˆ¶ 1: Template ç³»ç»Ÿ

**æ¨¡æ¿å˜é‡(TemplateVars)çš„ç”Ÿå‘½å‘¨æœŸ**:

```go
// 1ï¸âƒ£ Controller ä» ModelService æå–ä¸Šä¸‹æ–‡
type TemplateVars struct {
    ModelServiceName      string  // "llama2-7b"
    ModelName             string  // "meta-llama/Llama-2-7b-hf"
    HFModelName           string  // "meta-llama/Llama-2-7b-hf" (ä» uri: hf://... æå–)
    MountedModelPath      string  // "/model-cache" æˆ– "/model-cache/path/to/model"
    DecodeDeploymentName  string  // "llama2-7b-decode"
    // ... è¿˜æœ‰ 15+ ä¸ªå˜é‡
}

// 2ï¸âƒ£ æ¨¡æ¿å‡½æ•°(FuncMap)æä¾›è¾…åŠ©æŸ¥è¯¢
funcMap := template.FuncMap{
    "getPort": func(name string) int32 {
        // ä» ModelService.Spec.Routing.Ports æŸ¥æ‰¾
        for _, p := range msvc.Spec.Routing.Ports {
            if p.Name == name { return p.Port }
        }
        return -1
    },
}
```

**BaseConfig ä¸­çš„æ¨¡æ¿ä½¿ç”¨**:

```yaml
data:
  decodeDeployment: |
    spec:
      template:
        spec:
          containers:
          - name: vllm
            args:
            - "{{ .HFModelName }}"  # è¿è¡Œæ—¶æ›¿æ¢ä¸º "meta-llama/Llama-2-7b-hf"
            - "--port"
            - "{{ "app_port" | getPort }}"  # è°ƒç”¨å‡½æ•°æŸ¥æ‰¾ç«¯å£å· â†’ 8000
            env:
            - name: HF_HOME
              value: "{{ .MountedModelPath }}"  # â†’ "/model-cache"
```

**å…³é”®è®¾è®¡**:
- âœ… **å¹³å°å±‚æŠ½è±¡**: BaseConfig ä¸hardcode æ¨¡å‹å,ç”¨å˜é‡å ä½
- âœ… **ç±»å‹å®‰å…¨**: å‡½æ•°è¿”å›é”™è¯¯æ—¶ Controller æ‹’ç» Reconcile
- âœ… **è°ƒè¯•å‹å¥½**: Template æ¸²æŸ“å¤±è´¥ä¼šåœ¨ Event ä¸­æš´éœ²é”™è¯¯

---

### æ ¸å¿ƒæœºåˆ¶ 2: Merge ç­–ç•¥(è¯­ä¹‰åˆå¹¶)

**å¤–å–å¹³å°ç±»æ¯”**: å®¢æˆ·åœ¨æ ‡å‡†èœå•ä¸Šå‹¾é€‰"åŠ è¾£"ã€"ä¸è¦é¦™èœ",æœ€ç»ˆèœå“ = æ ‡å‡†é…æ–¹ + å®¢æˆ·å®šåˆ¶ã€‚

**æŠ€æœ¯å®ç°**: **DeepCopy + Semantic Overlay**

```yaml
# ğŸ“– BaseConfig å®šä¹‰(æ ‡å‡†é…æ–¹)
decodeDeployment: |
  spec:
    replicas: 1  # é»˜è®¤ 1 ä¸ªå‰¯æœ¬
    template:
      spec:
        containers:
        - name: vllm
          image: ghcr.io/llm-d/llm-d:0.0.8
          resources:
            limits:
              nvidia.com/gpu: 1
              cpu: "16"
              memory: 32Gi

---
# ğŸª ModelService è¦†ç›–(å®¢æˆ·å®šåˆ¶)
spec:
  decode:
    replicas: 3  # è¦†ç›–: 3 ä¸ªå‰¯æœ¬
    containers:
    - name: vllm
      resources:
        limits:
          nvidia.com/gpu: 2  # è¦†ç›–: 2 å¼  GPU
          # âš ï¸ æ³¨æ„: æ²¡æœ‰æŒ‡å®š cpu/memory,ä¼šä¿ç•™ BaseConfig çš„å€¼
```

**åˆå¹¶ç»“æœ(Controller è‡ªåŠ¨è®¡ç®—)**:

```yaml
# âœ… æœ€ç»ˆ Deployment
spec:
  replicas: 3  # â† ModelService è¦†ç›–
  template:
    spec:
      containers:
      - name: vllm
        image: ghcr.io/llm-d/llm-d:0.0.8  # â† BaseConfig ä¿ç•™
        resources:
          limits:
            nvidia.com/gpu: 2  # â† ModelService è¦†ç›–
            cpu: "16"  # â† BaseConfig ä¿ç•™(å› ä¸º ModelService æ²¡æŒ‡å®š)
            memory: 32Gi  # â† BaseConfig ä¿ç•™
```

**Merge Transformers ä»£ç é€»è¾‘**:

```go
func mergeContainers(baseContainers, msvcContainers []ContainerSpec) []ContainerSpec {
    result := deepCopy(baseContainers)  // 1ï¸âƒ£ æ·±æ‹·è´ BaseConfig
    
    for _, msvcC := range msvcContainers {
        found := false
        for i, baseC := range result {
            if baseC.Name == msvcC.Name {  // 2ï¸âƒ£ æŒ‰ Name åŒ¹é…å®¹å™¨
                // 3ï¸âƒ£ è¯­ä¹‰åˆå¹¶: åªè¦†ç›– ModelService æŒ‡å®šçš„å­—æ®µ
                if msvcC.Image != nil {
                    result[i].Image = msvcC.Image
                }
                if len(msvcC.Args) > 0 {
                    result[i].Args = msvcC.Args
                }
                mergeResources(&result[i].Resources, msvcC.Resources)
                found = true
                break
            }
        }
        if !found {
            result = append(result, msvcC)  // 4ï¸âƒ£ æ–°å®¹å™¨ç›´æ¥è¿½åŠ 
        }
    }
    return result
}
```

**å…³é”®è®¾è®¡**:
- âœ… **æŒ‰å­—æ®µè¦†ç›–**: ä¸æ˜¯"å…¨æœ‰æˆ–å…¨æ— ",è€Œæ˜¯ç²¾ç»†åˆ°æ¯ä¸ªå­—æ®µ
- âœ… **åˆ—è¡¨åˆå¹¶æ™ºèƒ½**: Containers æŒ‰ `name` åŒ¹é…,Env æŒ‰ `name` å»é‡
- âœ… **ç©ºå€¼è¯­ä¹‰**: ModelService çš„ç©ºå€¼ = "ä¸è¦†ç›–",è€Œé"è®¾ä¸ºç©º"

---

### æ ¸å¿ƒæœºåˆ¶ 3: èµ„æºä¾èµ–å›¾ä¸ OwnerReference

**å¤–å–å¹³å°ç±»æ¯”**: è®¢å•æ˜¯"è€æ¿",å¨æˆ¿ã€çª—å£ã€é…é€å‘˜éƒ½æ˜¯"å‘˜å·¥"ã€‚è®¢å•å–æ¶ˆæ—¶,æ‰€æœ‰ç›¸å…³èµ„æºè‡ªåŠ¨å›æ”¶ã€‚

```mermaid
graph TD
    MS["ModelService<br/>llama2-7b<br/>ğŸ‘‘ Owner"]
    
    MS -->|OwnerReference| DECODE_DEPLOY["Decode Deployment"]
    MS -->|OwnerReference| DECODE_SVC["Decode Service"]
    MS -->|OwnerReference| PREFILL_DEPLOY["Prefill Deployment"]
    MS -->|OwnerReference| PREFILL_SVC["Prefill Service"]
    MS -->|OwnerReference| EPP_DEPLOY["EPP Deployment"]
    MS -->|OwnerReference| EPP_SVC["EPP Service"]
    MS -->|OwnerReference| HTTPROUTE["HTTPRoute"]
    MS -->|OwnerReference| POOL["InferencePool"]
    MS -->|OwnerReference| MODEL["InferenceModel"]
    MS -->|OwnerReference| SA["ServiceAccount"]
    MS -->|OwnerReference| RB["RoleBinding"]
    
    style MS fill:#fff9c4,stroke:#f57f17,stroke-width:3px
    style DECODE_DEPLOY fill:#e8f5e9
    style DECODE_SVC fill:#e8f5e9
    style PREFILL_DEPLOY fill:#e8f5e9
    style PREFILL_SVC fill:#e8f5e9
    style EPP_DEPLOY fill:#e8f5e9
    style EPP_SVC fill:#e8f5e9
    style HTTPROUTE fill:#e1f5fe
    style POOL fill:#e1f5fe
    style MODEL fill:#e1f5fe
    style SA fill:#fce4ec
    style RB fill:#fce4ec
```

**æŠ€æœ¯å®ç°**: Kubernetes OwnerReference

```go
// Controller åˆ›å»ºå­èµ„æºæ—¶è‡ªåŠ¨æ³¨å…¥
deployment := &appsv1.Deployment{
    ObjectMeta: metav1.ObjectMeta{
        Name:      "llama2-7b-decode",
        Namespace: modelService.Namespace,
        OwnerReferences: []metav1.OwnerReference{
            *metav1.NewControllerRef(modelService, schema.GroupVersionKind{
                Group:   "llm-d.ai",
                Version: "v1alpha1",
                Kind:    "ModelService",
            }),
        },
    },
    // ...
}
```

**æ”¶ç›Š**:
- âœ… **çº§è”åˆ é™¤**: `kubectl delete modelservice llama2-7b` è‡ªåŠ¨æ¸…ç†æ‰€æœ‰å…³è”èµ„æº
- âœ… **æ‰€æœ‰æƒè¿½è¸ª**: `kubectl get deployment -o yaml` å¯æŸ¥çœ‹å±äºå“ªä¸ª ModelService
- âœ… **Reconcile è¾¹ç•Œ**: Controller åªç®¡ç†"è‡ªå·±åˆ›å»ºçš„"èµ„æº,é¿å…è¯¯æ“ä½œç”¨æˆ·æ‰‹åŠ¨åˆ›å»ºçš„åŒåèµ„æº

---

### æ ¸å¿ƒæœºåˆ¶ 4: æ¨¡å‹åŠ è½½è·¯å¾„é€‚é…

**ä¸‰ç§é£Ÿæé‡‡è´­è·¯å¾„(å¤–å–å¹³å°ç±»æ¯”)**:

| é‡‡è´­æ–¹å¼ | ç±»æ¯” | llm-d-modelservice | Volume ç±»å‹ |
|---------|------|-------------------|------------|
| **HuggingFace** | ä»æ‰¹å‘å¸‚åœºç°åœºé‡‡è´­ | `uri: hf://meta-llama/Llama-2-7b-hf` | emptyDir + HF ä¸‹è½½ |
| **PVC** | ä»è‡ªå®¶å†·åº“æè´§ | `uri: pvc://model-pvc/path/to/model` | PersistentVolumeClaim |
| **OCI** | ä»ä¾›åº”å•†å†·é“¾è½¦æè´§ | `uri: oci://ghcr.io/models/llama2:v1` | Image Volume (K8s 1.31+) |

**HuggingFace è·¯å¾„çš„è‡ªåŠ¨åŒ–æµç¨‹**:

```yaml
spec:
  modelArtifacts:
    uri: hf://meta-llama/Llama-2-7b-hf
    authSecretName: hf-token-secret  # å¯é€‰,gated æ¨¡å‹éœ€è¦
    size: 50Gi  # emptyDir å¤§å°
```

**Controller è‡ªåŠ¨ç”Ÿæˆçš„èµ„æº**:

```yaml
# 1ï¸âƒ£ åˆ›å»º emptyDir Volume
volumes:
- name: model-storage
  emptyDir:
    sizeLimit: 50Gi

# 2ï¸âƒ£ æŒ‚è½½åˆ°å®¹å™¨
containers:
- name: vllm
  volumeMounts:
  - name: model-storage
    mountPath: /model-cache  # â† æš´éœ²ä¸º {{ .MountedModelPath }}
  
  env:
  - name: HF_HOME
    value: /model-cache  # vLLM è‡ªåŠ¨ä» HF ä¸‹è½½åˆ°è¿™é‡Œ
  - name: HF_TOKEN  # å¦‚æœæŒ‡å®šäº† authSecretName
    valueFrom:
      secretKeyRef:
        name: hf-token-secret
        key: HF_TOKEN
  
  args:
  - "{{ .HFModelName }}"  # â†’ "meta-llama/Llama-2-7b-hf"
```

**PVC è·¯å¾„çš„ä¼˜åŒ–**:

```yaml
spec:
  modelArtifacts:
    uri: pvc://granite-pvc/models/granite-7b
```

**è‡ªåŠ¨ç”Ÿæˆçš„ Volume**:

```yaml
volumes:
- name: model-storage
  persistentVolumeClaim:
    claimName: granite-pvc
    readOnly: true  # âœ… åªè¯»æŒ‚è½½,é˜²æ­¢è¯¯ä¿®æ”¹

containers:
- name: vllm
  volumeMounts:
  - name: model-storage
    mountPath: /model-cache
    readOnly: true
  
  args:
  - "{{ .MountedModelPath }}"  # â†’ "/model-cache/models/granite-7b"
```

---

### æ—¶åºå›¾: å®Œæ•´ Reconcile å‘¨æœŸ

```mermaid
sequenceDiagram
    participant User as ğŸ‘¤ ç”¨æˆ·
    participant API as K8s API
    participant Ctrl as Controller
    participant BC as BaseConfig<br/>ConfigMap
    participant Deploy as Deployment
    participant Status as ModelService<br/>.status
    
    User->>API: kubectl apply modelservice.yaml
    API->>Ctrl: ğŸ”” Event: Create
    
    rect rgb(255, 249, 196)
        Note over Ctrl: Reconcile å¼€å§‹
        
        Ctrl->>BC: Get BaseConfig
        BC-->>Ctrl: YAML æ¨¡æ¿
        
        Ctrl->>Ctrl: å¡«å……æ¨¡æ¿å˜é‡
        Note over Ctrl: HFModelName, Ports, etc.
        
        Ctrl->>Ctrl: åˆå¹¶ Base + ModelService
        Note over Ctrl: DeepCopy + Overlay
        
        Ctrl->>API: Get Existing Deployment
        API-->>Ctrl: Not Found (é¦–æ¬¡åˆ›å»º)
        
        Ctrl->>Deploy: Create Deployment
        Deploy-->>Ctrl: âœ… Created
        
        Ctrl->>API: Create Service, HTTPRoute, etc.
        API-->>Ctrl: âœ… All Created
        
        Ctrl->>Status: Update .status.decodeReady
        Note over Ctrl: "0/3" (Pod è¿˜åœ¨å¯åŠ¨)
    end
    
    Note over Ctrl: ç­‰å¾… 30s (Requeue)
    
    rect rgb(255, 249, 196)
        Note over Ctrl: ç¬¬ 2 æ¬¡ Reconcile
        
        Ctrl->>Deploy: Get Deployment Status
        Deploy-->>Ctrl: Ready: 3/3, Available: 3
        
        Ctrl->>Status: Update .status.decodeReady = "3/3"
        Ctrl->>Status: Update .status.decodeAvailable = 3
    end
    
    API-->>User: kubectl get modelservice<br/>DECODE READY: 3/3 âœ…
```

---

### âœ… èºæ—‹ 2 éªŒæ”¶æ ‡å‡†

å®Œæˆæœ¬å±‚å­¦ä¹ å,ä½ åº”è¯¥èƒ½å¤Ÿ:

1. **ç”»å‡º Reconciliation æµç¨‹å›¾**:  
   Watch Event â†’ Get BaseConfig â†’ Fill Template â†’ Merge â†’ Apply Resources â†’ Update Status

2. **è§£é‡Šæ¨¡æ¿å˜é‡å¡«å……**:  
   `{{ .HFModelName }}` å¦‚ä½•ä» `uri: hf://...` æå–? `{{ "app_port" | getPort }}` å¦‚ä½•æŸ¥æ‰¾?

3. **ç†è§£åˆå¹¶ä¼˜å…ˆçº§**:  
   ModelService å’Œ BaseConfig åŒæ—¶å®šä¹‰ `replicas`,è°ç”Ÿæ•ˆ? (ç­”: ModelService)

4. **æ¨å¯¼èµ„æºç”Ÿå‘½å‘¨æœŸ**:  
   åˆ é™¤ ModelService æ—¶,ä¸ºä»€ä¹ˆ Deployment ä¹Ÿä¼šè¢«åˆ é™¤? (ç­”: OwnerReference çº§è”åˆ é™¤)

---

### ğŸ”— ä¸‹ä¸€æ­¥æŒ‡å¼•

- **æƒ³çœ‹å®æˆ˜é…ç½®?** â†’ è¿›å…¥ [èºæ—‹ 3: ç”Ÿäº§çº§æ¨ç†æœåŠ¡é…ç½®](#-èºæ—‹-3-å®æˆ˜å±‚---ç”Ÿäº§çº§æ¨ç†æœåŠ¡é…ç½®)
- **æ·±å…¥ Controller å®ç°?** â†’ é˜…è¯» [æ§åˆ¶å™¨åè°ƒå¾ªç¯](./components/controller-reconciliation.md)
- **ç†è§£ Prefill/Decode æœºåˆ¶?** â†’ é˜…è¯» [Prefill/Decode åˆ†ç¦»æ¶æ„](./components/prefill-decode-arch.md)

---

## ğŸŒ€ èºæ—‹ 3: å®æˆ˜å±‚ - ç”Ÿäº§çº§æ¨ç†æœåŠ¡é…ç½®

### æœ¬å±‚ç›®æ ‡
æŒæ¡ä» Simple Model åˆ° Prefill/Decode åˆ†ç¦»çš„æ¸è¿›é…ç½®,ç†è§£ BaseConfig é¢„è®¾è®¾è®¡ã€æˆæœ¬ä¼˜åŒ–ç­–ç•¥ã€æ•…éšœæ’éšœæ–¹æ³•ã€‚

---

### åœºæ™¯ 1: å¿«é€Ÿä¸Šçº¿ HuggingFace æ¨¡å‹(Simple Model)

**éœ€æ±‚**: 10 åˆ†é’Ÿå†…ä¸Šçº¿ Llama2-7B,å• Pod éƒ¨ç½²,ä¸‹è½½è‡ª HuggingFaceã€‚

**Step 1: åˆ›å»º BaseConfig(å¹³å°å›¢é˜Ÿç»´æŠ¤)**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: simple-base-config
  namespace: llm-platform
immutable: true  # âš ï¸ ç”Ÿäº§ç¯å¢ƒå»ºè®®è®¾ä¸º immutable,é˜²æ­¢è¯¯ä¿®æ”¹
data:
  decodeDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          containers:
          - name: vllm
            image: ghcr.io/llm-d/llm-d:0.0.8
            command: ["vllm", "serve"]
            args:
            - "--port"
            - "{{ "app_port" | getPort }}"
            env:
            - name: HF_HOME
              value: /cache
            volumeMounts:
            - name: model-cache
              mountPath: /cache
            resources:
              limits:
                nvidia.com/gpu: 1
              requests:
                cpu: "16"
                memory: 16Gi
                nvidia.com/gpu: 1
          volumes:
          - name: model-cache
            emptyDir:
              sizeLimit: 50Gi  # ç¡®ä¿è¶³å¤Ÿå­˜å‚¨æ¨¡å‹
  
  decodeService: |
    apiVersion: v1
    kind: Service
    spec:
      clusterIP: None  # Headless Service,ç”¨äº StatefulSet åœºæ™¯
      ports:
      - name: vllm
        port: {{ "app_port" | getPort }}
        protocol: TCP
```

**Step 2: åˆ›å»º ModelService(æ¨¡å‹å›¢é˜Ÿç»´æŠ¤)**

```yaml
apiVersion: llm-d.ai/v1alpha1
kind: ModelService
metadata:
  name: llama2-7b
  namespace: ai-inference
spec:
  decoupleScaling: false  # Controller ç®¡ç†å‰¯æœ¬æ•°(ä¸ä½¿ç”¨ HPA)
  
  baseConfigMapRef:
    name: simple-base-config
  
  routing:
    modelName: meta-llama/Llama-2-7b-hf  # OpenAI å®¢æˆ·ç«¯è¯·æ±‚çš„ model å­—æ®µ
    ports:
    - name: app_port
      port: 8000
  
  modelArtifacts:
    uri: hf://meta-llama/Llama-2-7b-hf  # HuggingFace è‡ªåŠ¨ä¸‹è½½
  
  decode:
    replicas: 1
    containers:
    - name: vllm
      args:
      - "{{ .HFModelName }}"  # æ¸²æŸ“ä¸º "meta-llama/Llama-2-7b-hf"
```

**Step 3: éªŒè¯éƒ¨ç½²**

```bash
# 1ï¸âƒ£ æ£€æŸ¥ ModelService çŠ¶æ€
kubectl get modelservice -n ai-inference
# NAME         DECOUPLE SCALING   PREFILL READY   DECODE READY   DECODE AVAIL   AGE
# llama2-7b    false              0/0             1/1            1              2m

# 2ï¸âƒ£ æ£€æŸ¥ Pod æ—¥å¿—(ç¡®è®¤æ¨¡å‹ä¸‹è½½æˆåŠŸ)
kubectl logs -n ai-inference $(kubectl get pod -n ai-inference -l app=llama2-7b-decode -o name) | grep "Loading"
# INFO:     Loading model meta-llama/Llama-2-7b-hf
# INFO:     Model loaded successfully

# 3ï¸âƒ£ æµ‹è¯•æ¨ç†(Port-Forward)
kubectl port-forward -n ai-inference svc/llama2-7b-decode-service 8000:8000

curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-hf",
    "prompt": "San Francisco is a",
    "max_tokens": 20
  }'
```

**ğŸ¯ æˆæœ¬åˆ†æ**:
- GPU: 1x A100 (80GB) = ~$3/å°æ—¶
- é€‚ç”¨åœºæ™¯: å¼€å‘æµ‹è¯•ã€ä½å¹¶å‘(<10 QPS)

---

### åœºæ™¯ 2: Prefill/Decode åˆ†ç¦»æ¶æ„(ç”Ÿäº§çº§)

**éœ€æ±‚**: ååé‡ >100 QPS,é•¿çŸ­è¯·æ±‚æ··åˆ,æˆæœ¬ä¼˜åŒ–ã€‚

**æ ¸å¿ƒç­–ç•¥**:
- Prefill: å°‘é‡é«˜é… GPU(A100-80GB),å¤„ç†é•¿ prompt
- Decode: å¤§é‡ä½é… GPU(A10G-24GB),å¿«é€Ÿç”Ÿæˆ token

```yaml
apiVersion: llm-d.ai/v1alpha1
kind: ModelService
metadata:
  name: llama2-70b-prod
  namespace: ai-inference
spec:
  decoupleScaling: true  # âš ï¸ å¯ç”¨ HPA/Custom Autoscaler ç®¡ç†å‰¯æœ¬æ•°
  
  baseConfigMapRef:
    name: pd-base-config  # å¼•ç”¨ Prefill/Decode ä¸“ç”¨é…ç½®
  
  routing:
    modelName: meta-llama/Llama-2-70b-hf
    ports:
    - name: prefill_port
      port: 9000
    - name: decode_port
      port: 9001
    - name: app_port
      port: 8000
  
  modelArtifacts:
    uri: pvc://llama2-70b-pvc/models/llama2-70b  # ä» PVC åŠ è½½,é¿å…ä¸‹è½½æ—¶é—´
  
  # ğŸ”§ Prefill: é«˜å†…å­˜ GPU,TP=8
  prefill:
    replicas: 2  # âš ï¸ decoupleScaling=true æ—¶æ­¤å€¼è¢«å¿½ç•¥,ç”± HPA ç®¡ç†
    parallelism:
      tensor: 8  # Tensor Parallelism,å•ä¸ªå®ä¾‹è·¨ 8 å¼ å¡
    acceleratorTypes:
      labelKey: nvidia.com/gpu.product
      labelValues: ["A100-80GB"]  # åªè°ƒåº¦åˆ° A100
    containers:
    - name: vllm
      args:
      - "{{ .MountedModelPath }}"
      - "--port"
      - "{{ "prefill_port" | getPort }}"
      - "--tensor-parallel-size"
      - "8"
      - "--role"
      - "prefill"
      resources:
        limits:
          nvidia.com/gpu: 8  # 8 å¼  A100
  
  # ğŸš€ Decode: é«˜åå GPU,TP=1
  decode:
    replicas: 10  # âš ï¸ åŒæ ·è¢« HPA ç®¡ç†
    parallelism:
      tensor: 1  # å•å¡å³å¯
    acceleratorTypes:
      labelKey: nvidia.com/gpu.product
      labelValues: ["A10G"]  # ä½¿ç”¨ä¾¿å®œçš„ A10G
    containers:
    - name: vllm
      args:
      - "{{ .MountedModelPath }}"
      - "--port"
      - "{{ "decode_port" | getPort }}"
      - "--tensor-parallel-size"
      - "1"
      - "--role"
      - "decode"
      resources:
        limits:
          nvidia.com/gpu: 1  # å•å¡ A10G
  
  # ğŸ¯ EPP(Endpoint Picker): æ™ºèƒ½è·¯ç”±
  endpointPicker:
    replicas: 3
    containers:
    - name: epp
      env:
      - name: PREFILL_ENDPOINT
        value: "http://{{ .PrefillServiceName }}:{{ "prefill_port" | getPort }}"
      - name: DECODE_ENDPOINT
        value: "http://{{ .DecodeServiceName }}:{{ "decode_port" | getPort }}"
```

**ğŸ¯ æˆæœ¬å¯¹æ¯”**:

| æ¶æ„ | GPU é…ç½® | æˆæœ¬/å°æ—¶ | ååé‡(QPS) | æˆæœ¬/1M Token |
|------|---------|----------|-------------|--------------|
| å•ä½“ | 10x A100-80GB | $30 | 50 | $600 |
| åˆ†ç¦» | 2x A100-80GB(Prefill) + 10x A10G(Decode) | $6 + $5 = $11 | 120 | $92 |

**ğŸ’° ä¼˜åŒ–æ”¶ç›Š**: æˆæœ¬é™ä½ 63%,ååæå‡ 140%

---

### åœºæ™¯ 3: å¤šç§Ÿæˆ·å¹³å° - BaseConfig é¢„è®¾è®¾è®¡

**éœ€æ±‚**: æ”¯æŒ 3 ç§æ¨¡å‹è§„æ ¼(å°ã€ä¸­ã€å¤§),æ¯ç§è§„æ ¼æœ‰ä¸åŒçš„èµ„æºé…ç½®ã€‚

**å¹³å°å›¢é˜Ÿç»´æŠ¤ 3 ä¸ª BaseConfig**:

```yaml
# 1ï¸âƒ£ small-model-preset
apiVersion: v1
kind: ConfigMap
metadata:
  name: small-model-preset
  namespace: llm-platform
data:
  decodeDeployment: |
    spec:
      template:
        spec:
          containers:
          - name: vllm
            resources:
              limits:
                nvidia.com/gpu: 1  # 1 å¼  T4/A10
                cpu: "8"
                memory: 16Gi

---
# 2ï¸âƒ£ medium-model-preset
apiVersion: v1
kind: ConfigMap
metadata:
  name: medium-model-preset
data:
  decodeDeployment: |
    spec:
      template:
        spec:
          containers:
          - name: vllm
            resources:
              limits:
                nvidia.com/gpu: 2  # 2 å¼  A10G
                cpu: "16"
                memory: 32Gi

---
# 3ï¸âƒ£ large-model-preset
apiVersion: v1
kind: ConfigMap
metadata:
  name: large-model-preset
data:
  decodeDeployment: |
    spec:
      template:
        spec:
          containers:
          - name: vllm
            resources:
              limits:
                nvidia.com/gpu: 8  # 8 å¼  A100
                cpu: "64"
                memory: 256Gi
```

**æ¨¡å‹å›¢é˜Ÿä½¿ç”¨**:

```yaml
# å°æ¨¡å‹å›¢é˜Ÿ
apiVersion: llm-d.ai/v1alpha1
kind: ModelService
metadata:
  name: qwen-7b
spec:
  baseConfigMapRef:
    name: small-model-preset  # å¼•ç”¨å°è§„æ ¼é¢„è®¾
  # ... å…¶ä»–é…ç½®

---
# å¤§æ¨¡å‹å›¢é˜Ÿ
apiVersion: llm-d.ai/v1alpha1
kind: ModelService
metadata:
  name: llama2-70b
spec:
  baseConfigMapRef:
    name: large-model-preset  # å¼•ç”¨å¤§è§„æ ¼é¢„è®¾
  # ... å…¶ä»–é…ç½®
```

**ğŸ¯ æ”¶ç›Š**:
- âœ… æ¨¡å‹å›¢é˜Ÿæ— éœ€ç†è§£ GPU èµ„æºé…ç½®
- âœ… å¹³å°ç»Ÿä¸€å‡çº§(å¦‚è°ƒæ•´ CPU é…é¢)åªéœ€ä¿®æ”¹ BaseConfig
- âœ… æˆæœ¬å®¡è®¡: æŒ‰ BaseConfig åˆ†ç±»ç»Ÿè®¡è´¹ç”¨

---

### åœºæ™¯ 4: å¼¹æ€§ä¼¸ç¼© - DecoupleScaling + HPA

**éœ€æ±‚**: ç™½å¤©é«˜å³° 50 QPS,å¤œé—´ä½è°· 5 QPS,è‡ªåŠ¨æ‰©ç¼©å®¹ã€‚

**Step 1: å¯ç”¨ DecoupleScaling**

```yaml
spec:
  decoupleScaling: true  # âš ï¸ Controller ä¸å†ç®¡ç† replicas,äº¤ç»™ HPA
```

**Step 2: åˆ›å»º HPA**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llama2-7b-decode-hpa
  namespace: ai-inference
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llama2-7b-decode  # â† ç”± ModelService åˆ›å»ºçš„ Deployment
  
  minReplicas: 2
  maxReplicas: 10
  
  metrics:
  - type: Pods
    pods:
      metric:
        name: vllm_queue_length  # è‡ªå®šä¹‰æŒ‡æ ‡: vLLM é˜Ÿåˆ—é•¿åº¦
      target:
        type: AverageValue
        averageValue: "10"  # æ¯ä¸ª Pod é˜Ÿåˆ— >10 æ—¶æ‰©å®¹
  
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60  # æ‰©å®¹å‰è§‚å¯Ÿ 60s
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60  # æ¯åˆ†é’Ÿæœ€å¤šæ‰©å®¹ 50%
    scaleDown:
      stabilizationWindowSeconds: 300  # ç¼©å®¹å‰è§‚å¯Ÿ 5 åˆ†é’Ÿ
      policies:
      - type: Pods
        value: 1
        periodSeconds: 120  # æ¯ 2 åˆ†é’Ÿæœ€å¤šç¼©å®¹ 1 ä¸ª Pod
```

**âš ï¸ å…³é”®è®¾è®¡**:
- `decoupleScaling: true` å,ModelService.Spec.Decode.Replicas å¤±æ•ˆ
- HPA ç›´æ¥ç®¡ç† Deployment å‰¯æœ¬æ•°
- Status å­—æ®µä»ç”± Controller æ›´æ–°(åæ˜ å®é™… Pod çŠ¶æ€)

---

### æ•…éšœæ’éšœ: ä» Status è¿½è¸ªé—®é¢˜

**1ï¸âƒ£ æ£€æŸ¥ ModelService Status**

```bash
kubectl get modelservice llama2-7b -o yaml
```

```yaml
status:
  # èµ„æºå¼•ç”¨
  decodeDeploymentRef: llama2-7b-decode
  decodeServiceRef: llama2-7b-decode-service
  httpRouteRef: llama2-7b-route
  inferenceModelRef: llama2-7b-model
  inferencePoolRef: llama2-7b-pool
  
  # Pod çŠ¶æ€
  decodeReady: "2/3"  # âš ï¸ åªæœ‰ 2 ä¸ª Pod Ready
  decodeAvailable: 2
  
  # Condition(ç±»ä¼¼ Event)
  conditions:
  - type: DecodeAvailable
    status: "False"
    reason: MinimumReplicasUnavailable
    message: "Deployment llama2-7b-decode has minimum availability."
    lastTransitionTime: "2025-02-07T11:00:00Z"
```

**æ’éšœè·¯å¾„**:

```mermaid
flowchart TD
    A[ModelService.Status<br/>decodeReady: 2/3] --> B{æ£€æŸ¥ Deployment}
    B --> C[kubectl describe deploy llama2-7b-decode]
    
    C --> D{Pod çŠ¶æ€?}
    D -->|Pending| E[æ£€æŸ¥èµ„æºä¸è¶³<br/>kubectl describe pod]
    D -->|CrashLoopBackOff| F[æ£€æŸ¥æ—¥å¿—<br/>kubectl logs]
    D -->|ImagePullBackOff| G[æ£€æŸ¥é•œåƒ/Secret<br/>kubectl get events]
    
    E --> H[æ’æŸ¥ GPU/CPU é…é¢<br/>æˆ– NodeSelector ä¸åŒ¹é…]
    F --> I[æ’æŸ¥ vLLM å¯åŠ¨å¤±è´¥<br/>æ¨¡å‹è·¯å¾„/æ˜¾å­˜ä¸è¶³]
    G --> J[æ’æŸ¥é•œåƒä»“åº“è®¤è¯<br/>æˆ–é•œåƒä¸å­˜åœ¨]
    
    style A fill:#ffcccc
    style H fill:#ccffcc
    style I fill:#ccffcc
    style J fill:#ccffcc
```

**2ï¸âƒ£ å¸¸è§é—®é¢˜å®šä½**

| ç—‡çŠ¶ | åŸå›  | æ’æŸ¥å‘½ä»¤ |
|------|------|---------|
| `decodeReady: 0/3` æŒç»­ 5 åˆ†é’Ÿ | GPU èµ„æºä¸è¶³,Pod Pending | `kubectl describe pod -l app=llama2-7b-decode` |
| Pod CrashLoopBackOff | æ¨¡å‹è·¯å¾„é”™è¯¯æˆ–æ˜¾å­˜ä¸è¶³ | `kubectl logs <pod-name> --previous` |
| Status å­—æ®µä¸ºç©º | BaseConfig ä¸å­˜åœ¨ | `kubectl get cm simple-base-config -n llm-platform` |
| HTTPRoute æœªåˆ›å»º | `routing.modelName` å†²çª | `kubectl get inferencemodel --all-namespaces` |

**3ï¸âƒ£ è°ƒè¯•æŠ€å·§**

```bash
# æŸ¥çœ‹ Controller æ—¥å¿—
kubectl logs -n llm-d-system deployment/modelservice-controller | grep "llama2-7b"

# æŸ¥çœ‹ Reconcile äº‹ä»¶
kubectl get events -n ai-inference --sort-by='.lastTimestamp' | grep ModelService

# æ‰‹åŠ¨è§¦å‘ Reconcile(ä¿®æ”¹ annotation)
kubectl annotate modelservice llama2-7b reconcile="$(date +%s)" --overwrite
```

---

### ç”Ÿäº§çº§é…ç½® Checklist

- [ ] **BaseConfig è®¾è®¡**
  - [ ] `immutable: true` é˜²æ­¢è¯¯ä¿®æ”¹
  - [ ] æ¨¡æ¿å˜é‡è¦†ç›–æ‰€æœ‰å¹³å°çº§å‚æ•°
  - [ ] é¢„è®¾ 3+ å¥—è§„æ ¼(å°/ä¸­/å¤§)

- [ ] **ModelService é…ç½®**
  - [ ] `routing.modelName` å…¨å±€å”¯ä¸€
  - [ ] `modelArtifacts.size` è¶³å¤Ÿå­˜å‚¨æ¨¡å‹
  - [ ] `decoupleScaling: true` + HPA for ç”Ÿäº§ç¯å¢ƒ

- [ ] **èµ„æºé…é¢**
  - [ ] è®¾ç½® ResourceQuota é™åˆ¶å‘½åç©ºé—´æ€» GPU æ•°
  - [ ] PodDisruptionBudget ä¿è¯é«˜å¯ç”¨(minAvailable: 50%)

- [ ] **ç›‘æ§å‘Šè­¦**
  - [ ] Status.decodeReady æŒç»­ä¸å¥åº· >5min å‘Šè­¦
  - [ ] vLLM queue_length >50 å‘Šè­¦
  - [ ] GPU åˆ©ç”¨ç‡ <30% æˆæœ¬æµªè´¹å‘Šè­¦

- [ ] **å¤‡ä»½æ¢å¤**
  - [ ] ModelService YAML çº³å…¥ GitOps
  - [ ] PVC å®šæœŸå¿«ç…§(å¦‚ä½¿ç”¨ Velero)

---

### åæ¨¡å¼(Anti-Patterns)

| âŒ åæ¨¡å¼ | âœ… æ­£ç¡®åšæ³• | åŸå›  |
|---------|-----------|------|
| ç›´æ¥ä¿®æ”¹ Controller åˆ›å»ºçš„ Deployment | ä¿®æ”¹ ModelService,è®© Controller æ›´æ–° | æ‰‹åŠ¨æ”¹åŠ¨ä¼šè¢« Reconcile è¦†ç›– |
| å¤šä¸ª ModelService ä½¿ç”¨ç›¸åŒ `routing.modelName` | ç¡®ä¿å…¨å±€å”¯ä¸€,æˆ–ä½¿ç”¨å‘½åç©ºé—´éš”ç¦» | å¯¼è‡´è·¯ç”±å†²çª,æ—§ ModelService è¢«æ ‡è®° NotReady |
| `decoupleScaling: false` + æ‰‹åŠ¨åˆ›å»º HPA | è®¾ç½® `decoupleScaling: true` | Controller å’Œ HPA æŠ¢å  replicas æ§åˆ¶æƒ |
| BaseConfig é¢‘ç¹ä¿®æ”¹ | ä½¿ç”¨ç‰ˆæœ¬åŒ–(å¦‚ `vllm-v1`, `vllm-v2`) | immutable ConfigMap ä¿®æ”¹åä¸ç”Ÿæ•ˆ,éœ€åˆ é™¤é‡å»º |
| emptyDir å­˜å‚¨å¤§æ¨¡å‹(>100GB) | ä½¿ç”¨ PVC | emptyDir å ç”¨èŠ‚ç‚¹ç£ç›˜,å½±å“å…¶ä»– Pod |

---

### âœ… èºæ—‹ 3 éªŒæ”¶æ ‡å‡†

å®Œæˆæœ¬å±‚å­¦ä¹ å,ä½ åº”è¯¥èƒ½å¤Ÿ:

1. **ç‹¬ç«‹é…ç½®ç”Ÿäº§çº§ ModelService**:  
   ä» Simple Model â†’ Prefill/Decode åˆ†ç¦» â†’ å¼¹æ€§ä¼¸ç¼©,å®Œæ•´é…ç½®æµç¨‹

2. **æˆæœ¬ä¼˜åŒ–å†³ç­–**:  
   èƒ½è®¡ç®— Prefill/Decode åˆ†ç¦»æ¶æ„çš„æˆæœ¬æ”¶ç›Š,é€‰æ‹©åˆé€‚çš„ GPU ç±»å‹

3. **æ•…éšœæ’éšœè·¯å¾„**:  
   ä» Status â†’ Deployment â†’ Pod â†’ Logs,å¿«é€Ÿå®šä½æ ¹å› 

4. **BaseConfig è®¾è®¡åŸåˆ™**:  
   èƒ½ä¸ºå¤šç§Ÿæˆ·å¹³å°è®¾è®¡åˆç†çš„ BaseConfig é¢„è®¾ä½“ç³»

---

### ğŸ”— å»¶ä¼¸é˜…è¯»

- **[CRD è®¾è®¡å“²å­¦](./components/crd-design.md)**: ç†è§£ Spec å­—æ®µçš„è¯­ä¹‰è®¾è®¡
- **[æ§åˆ¶å™¨åè°ƒå¾ªç¯](./components/controller-reconciliation.md)**: æ·±å…¥ Reconcile é€»è¾‘ä¸é”™è¯¯å¤„ç†
- **[Prefill/Decode åˆ†ç¦»æ¶æ„](./components/prefill-decode-arch.md)**: EPP è·¯ç”±ç®—æ³•ä¸è´Ÿè½½å‡è¡¡
- **[æ¨¡å‹åŠ è½½ç­–ç•¥](./components/model-artifact-loading.md)**: In-cluster/Node-level ç¼“å­˜ä¼˜åŒ–

---

## ğŸ§© å­æ¨¡å—ç´¢å¼•

æœ¬æ–‡æ¡£é‡‡ç”¨**æ¨¡å—åŒ–æ¶æ„**,å°†å¤æ‚ç³»ç»Ÿæ‹†è§£ä¸ºç‹¬ç«‹é—­ç¯çš„å­æ–‡æ¡£:

| æ¨¡å— | æ ¸å¿ƒå…³æ³¨ç‚¹ | é€‚åˆäººç¾¤ |
|------|-----------|---------|
| [CRD è®¾è®¡å“²å­¦](./components/crd-design.md) | ModelService + BaseConfig çš„å£°æ˜å¼æŠ½è±¡ | å¹³å°æ¶æ„å¸ˆã€CRD å¼€å‘è€… |
| [æ§åˆ¶å™¨åè°ƒå¾ªç¯](./components/controller-reconciliation.md) | Reconciliation Loop + æ¨¡æ¿ç³»ç»Ÿ + èµ„æºåˆå¹¶ | Operator å¼€å‘è€…ã€SRE |
| [Prefill/Decode åˆ†ç¦»æ¶æ„](./components/prefill-decode-arch.md) | å¼‚æ„ç®—åŠ›ä¼˜åŒ– + EPP è·¯ç”±ç­–ç•¥ | AI æ¨ç†å·¥ç¨‹å¸ˆã€æ€§èƒ½ä¼˜åŒ–ä¸“å®¶ |
| [æ¨¡å‹åŠ è½½ç­–ç•¥](./components/model-artifact-loading.md) | HuggingFace/PVC/OCI å¤šæºé€‚é… | æ¨¡å‹éƒ¨ç½²å·¥ç¨‹å¸ˆã€DevOps |

---

## ğŸ“š ç›¸å…³èµ„æº

- **ä¸Šæ¸¸é¡¹ç›®**: [llm-d/llm-d](https://github.com/llm-d/llm-d)
- **Gateway API Inference Extension**: [GIE Spec](https://gateway-api-inference-extension.sigs.k8s.io)
- **åºŸå¼ƒè¯´æ˜**: æœ¬é¡¹ç›®å·²è¿ç§»è‡³ Helm Chart æ–¹æ¡ˆ,è¯¦è§ [llm-d-incubation/llm-d-modelservice](https://github.com/llm-d-incubation/llm-d-modelservice)
